"""
Utilitare pentru procesarea fiÈ™ierelor JSON chunkizate
Optimizat pentru formatul EXACT specificat de utilizator
"""

import json
import os
import logging
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
from pathlib import Path

logger = logging.getLogger(__name__)

def validate_json_format(file_path: str) -> Tuple[bool, str, int]:
    """
    ValideazÄƒ cÄƒ fiÈ™ierul JSON are formatul EXACT specificat:
    {
        "chunk_0": {
            "metadata": "string",
            "chunk": "content"
        }
    }
    
    Returns:
        (is_valid, error_message, chunks_count)
    """
    try:
        if not os.path.exists(file_path):
            return False, "FiÈ™ierul nu existÄƒ", 0
        
        if os.path.getsize(file_path) == 0:
            return False, "FiÈ™ierul este gol", 0
        
        # ÃncercÄƒm sÄƒ Ã®ncÄƒrcÄƒm JSON-ul
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError as e:
                return False, f"JSON invalid: {str(e)}", 0
        
        # VerificÄƒm cÄƒ este un dicÈ›ionar
        if not isinstance(data, dict):
            return False, "JSON-ul trebuie sÄƒ fie un obiect (dicÈ›ionar), nu o listÄƒ", 0
        
        # CÄƒutÄƒm chunk-uri Ã®n formatul chunk_0, chunk_1, etc.
        chunk_keys = [key for key in data.keys() if key.startswith('chunk_')]
        
        if len(chunk_keys) == 0:
            return False, "Nu s-au gÄƒsit chunk-uri Ã®n formatul aÈ™teptat (chunk_0, chunk_1, etc.)", 0
        
        # ValidÄƒm primele 3 chunk-uri pentru structura EXACTÄ‚
        valid_chunks = 0
        for key in chunk_keys[:3]:
            chunk = data[key]
            
            # VerificÄƒm cÄƒ chunk-ul este un dicÈ›ionar
            if not isinstance(chunk, dict):
                continue
            
            # VerificÄƒm cÄƒ are EXACT cheile: "metadata" È™i "chunk"
            if "metadata" not in chunk or "chunk" not in chunk:
                continue
            
            # VerificÄƒm cÄƒ metadata este STRING (nu dict!)
            if not isinstance(chunk["metadata"], str):
                continue
            
            # VerificÄƒm cÄƒ chunk este STRING cu conÈ›inut
            if not isinstance(chunk["chunk"], str) or len(chunk["chunk"].strip()) < 10:
                continue
            
            valid_chunks += 1
        
        if valid_chunks == 0:
            return False, "Nu s-au gÄƒsit chunk-uri valide cu structura EXACTÄ‚ (metadata: string, chunk: string)", 0
        
        logger.info(f"âœ… JSON valid gÄƒsit cu {len(chunk_keys)} chunk-uri, {valid_chunks} validate")
        return True, f"Valid - {len(chunk_keys)} chunk-uri gÄƒsite", len(chunk_keys)
        
    except Exception as e:
        logger.error(f"Eroare la validarea JSON: {str(e)}")
        return False, f"Eroare la validare: {str(e)}", 0

def process_json_chunks(file_path: str) -> List[Dict[str, Any]]:
    """
    ProceseazÄƒ fiÈ™ierul JSON cu structura EXACTÄ‚ È™i extrage chunk-urile pentru stocare
    
    Returns:
        Lista de dicÈ›ionare cu chunk-uri procesate
    """
    try:
        # ValidÄƒm mai Ã®ntÃ¢i formatul
        is_valid, error_msg, chunks_count = validate_json_format(file_path)
        if not is_valid:
            raise ValueError(f"FiÈ™ier JSON invalid: {error_msg}")
        
        # ÃncÄƒrcÄƒm datele
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Extragem chunk-urile
        chunks_data = []
        chunk_keys = sorted([key for key in data.keys() if key.startswith('chunk_')])
        
        for i, key in enumerate(chunk_keys):
            chunk = data[key]
            
            # VerificÄƒm structura EXACTÄ‚
            if not isinstance(chunk, dict) or "metadata" not in chunk or "chunk" not in chunk:
                logger.warning(f"Chunk invalid sÄƒrit: {key}")
                continue
            
            content = chunk["chunk"]
            metadata_string = chunk["metadata"]
            
            # VerificÄƒm cÄƒ sunt string-uri valide
            if not isinstance(content, str) or not isinstance(metadata_string, str):
                logger.warning(f"Chunk cu tipuri invalide sÄƒrit: {key}")
                continue
            
            # VerificÄƒm cÄƒ avem conÈ›inut suficient
            if len(content.strip()) < 10:
                logger.warning(f"Chunk cu conÈ›inut insuficient sÄƒrit: {key}")
                continue
            
            # CurÄƒÈ›Äƒm conÈ›inutul
            content = content.strip()
            
            # Convertim metadata string Ã®n dict pentru ChromaDB
            enhanced_metadata = {
                'chunk_id': key,
                'chunk_index': i,
                'content_length': len(content),
                'word_count': len(content.split()),
                'processed_at': datetime.now().isoformat(),
                'file_source': os.path.basename(file_path),
                'original_source': metadata_string,  # PÄƒstrÄƒm metadata originalÄƒ
                'source': metadata_string  # Pentru compatibilitate
            }
            
            chunks_data.append({
                'content': content,
                'metadata': enhanced_metadata
            })
        
        if not chunks_data:
            raise ValueError("Nu s-au putut extrage chunk-uri valide din fiÈ™ier")
        
        logger.info(f"âœ… Procesat cu succes: {len(chunks_data)} chunk-uri din {file_path}")
        return chunks_data
        
    except Exception as e:
        logger.error(f"Eroare la procesarea fiÈ™ierului {file_path}: {str(e)}")
        raise ValueError(f"Eroare neaÈ™teptatÄƒ la procesarea fiÈ™ierului: {str(e)}")

def get_json_statistics(file_path: str) -> Dict[str, Any]:
    """
    ObÈ›ine statistici despre fiÈ™ierul JSON chunkizat cu structura EXACTÄ‚
    
    Returns:
        DicÈ›ionar cu statistici despre fiÈ™ier
    """
    try:
        if not os.path.exists(file_path):
            return {"error": "FiÈ™ierul nu existÄƒ"}
        
        file_size = os.path.getsize(file_path)
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, dict):
            return {"error": "JSON-ul nu este un dicÈ›ionar"}
        
        # Analiza chunk-urilor cu structura EXACTÄ‚
        chunk_keys = [key for key in data.keys() if key.startswith('chunk_')]
        
        if not chunk_keys:
            return {"error": "Nu s-au gÄƒsit chunk-uri"}
        
        # Statistici detaliate
        total_content_length = 0
        total_words = 0
        valid_chunks = 0
        metadata_sources = set()
        
        for key in chunk_keys:
            chunk = data[key]
            if (isinstance(chunk, dict) and 
                "metadata" in chunk and 
                "chunk" in chunk and 
                isinstance(chunk["metadata"], str) and 
                isinstance(chunk["chunk"], str)):
                
                content = chunk["chunk"]
                metadata_string = chunk["metadata"]
                
                if len(content.strip()) >= 10:
                    valid_chunks += 1
                    total_content_length += len(content)
                    total_words += len(content.split())
                    metadata_sources.add(metadata_string)
        
        stats = {
            'file_name': os.path.basename(file_path),
            'file_size_bytes': file_size,
            'file_size_mb': round(file_size / (1024 * 1024), 2),
            'total_chunks': len(chunk_keys),
            'valid_chunks': valid_chunks,
            'invalid_chunks': len(chunk_keys) - valid_chunks,
            'total_content_length': total_content_length,
            'total_words': total_words,
            'average_chunk_length': round(total_content_length / valid_chunks) if valid_chunks > 0 else 0,
            'average_words_per_chunk': round(total_words / valid_chunks) if valid_chunks > 0 else 0,
            'unique_metadata_sources': len(metadata_sources),
            'metadata_sources_list': list(metadata_sources),
            'processed_at': datetime.now().isoformat()
        }
        
        logger.debug(f"ğŸ“Š Statistici generate pentru {file_path}: {valid_chunks} chunk-uri valide")
        return stats
        
    except Exception as e:
        logger.error(f"Eroare la generarea statisticilor pentru {file_path}: {str(e)}")
        return {"error": f"Eroare la analiza fiÈ™ierului: {str(e)}"}

def preview_json_chunks(file_path: str, max_chunks: int = 3) -> Dict[str, Any]:
    """
    OferÄƒ o previzualizare a chunk-urilor din fiÈ™ier cu structura EXACTÄ‚
    
    Args:
        file_path: Calea cÄƒtre fiÈ™ierul JSON
        max_chunks: NumÄƒrul maxim de chunk-uri de previzualizat
        
    Returns:
        DicÈ›ionar cu previzualizarea chunk-urilor
    """
    try:
        if not os.path.exists(file_path):
            return {"error": "FiÈ™ierul nu existÄƒ"}
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, dict):
            return {"error": "JSON-ul nu este un dicÈ›ionar"}
        
        chunk_keys = sorted([key for key in data.keys() if key.startswith('chunk_')])
        
        if not chunk_keys:
            return {"error": "Nu s-au gÄƒsit chunk-uri"}
        
        # SelectÄƒm primele chunk-uri pentru previzualizare
        preview_chunks = []
        
        for key in chunk_keys[:max_chunks]:
            chunk = data[key]
            
            if (isinstance(chunk, dict) and 
                "metadata" in chunk and 
                "chunk" in chunk and 
                isinstance(chunk["metadata"], str) and 
                isinstance(chunk["chunk"], str)):
                
                content = chunk["chunk"]
                metadata_string = chunk["metadata"]
                
                # TruncÄƒm conÈ›inutul pentru previzualizare
                preview_content = content[:200] + "..." if len(content) > 200 else content
                
                preview_chunks.append({
                    'chunk_id': key,
                    'content_preview': preview_content,
                    'content_length': len(content),
                    'word_count': len(content.split()),
                    'metadata': metadata_string
                })
        
        result = {
            'file_name': os.path.basename(file_path),
            'total_chunks': len(chunk_keys),
            'preview_chunks': preview_chunks,
            'showing': f"{min(len(preview_chunks), max_chunks)} din {len(chunk_keys)} chunk-uri"
        }
        
        logger.debug(f"ğŸ‘ï¸ Previzualizare generatÄƒ pentru {file_path}: {len(preview_chunks)} chunk-uri")
        return result
        
    except Exception as e:
        logger.error(f"Eroare la previzualizarea fiÈ™ierului {file_path}: {str(e)}")
        return {"error": f"Eroare la previzualizare: {str(e)}"}

def validate_chunk_structure(chunk_data: Dict[str, Any]) -> Tuple[bool, str]:
    """
    ValideazÄƒ structura EXACTÄ‚ a unui chunk individual
    
    Args:
        chunk_data: Datele chunk-ului de validat
        
    Returns:
        (is_valid, error_message)
    """
    try:
        if not isinstance(chunk_data, dict):
            return False, "Chunk-ul trebuie sÄƒ fie un dicÈ›ionar"
        
        # VerificÄƒm cheile EXACTE
        required_keys = ["metadata", "chunk"]
        for key in required_keys:
            if key not in chunk_data:
                return False, f"LipseÈ™te cheia obligatorie: {key}"
        
        # VerificÄƒm cÄƒ nu are chei Ã®n plus
        if set(chunk_data.keys()) != set(required_keys):
            return False, f"Chunk-ul trebuie sÄƒ aibÄƒ EXACT cheile: {required_keys}"
        
        # ValidÄƒm metadata - TREBUIE sÄƒ fie string
        metadata = chunk_data["metadata"]
        if not isinstance(metadata, str):
            return False, "Metadata trebuie sÄƒ fie string, nu dict sau alt tip"
        
        if len(metadata.strip()) == 0:
            return False, "Metadata nu poate fi string gol"
        
        # ValidÄƒm conÈ›inutul - TREBUIE sÄƒ fie string
        content = chunk_data["chunk"]
        if not isinstance(content, str):
            return False, "ConÈ›inutul chunk-ului trebuie sÄƒ fie string"
        
        if len(content.strip()) < 10:
            return False, "ConÈ›inutul chunk-ului este prea scurt (minimum 10 caractere)"
        
        return True, "Chunk valid cu structura exactÄƒ"
        
    except Exception as e:
        return False, f"Eroare la validarea chunk-ului: {str(e)}"

def clean_chunk_content(content: str) -> str:
    """
    CurÄƒÈ›Äƒ È™i normalizeazÄƒ conÈ›inutul unui chunk
    
    Args:
        content: ConÈ›inutul de curÄƒÈ›at
        
    Returns:
        ConÈ›inutul curÄƒÈ›at
    """
    if not isinstance(content, str):
        return ""
    
    # EliminÄƒm spaÈ›iile Ã®n plus
    content = content.strip()
    
    # Ãnlocuim multiple spaÈ›ii cu unul singur
    import re
    content = re.sub(r'\s+', ' ', content)
    
    # EliminÄƒm caracterele de control
    content = ''.join(char for char in content if ord(char) >= 32 or char in '\n\t')
    
    return content

def extract_metadata_info(metadata_string: str) -> Dict[str, Any]:
    """
    Extrage informaÈ›ii din metadata string È™i le converteÈ™te Ã®n dict
    
    Args:
        metadata_string: Metadata originalÄƒ ca string
        
    Returns:
        Metadata convertitÄƒ Ã®n dict
    """
    if not isinstance(metadata_string, str):
        return {"original_source": "Necunoscut"}
    
    # ÃncercÄƒm sÄƒ parsÄƒm informaÈ›ii din string
    metadata_dict = {
        "original_source": metadata_string,
        "source": metadata_string
    }
    
    # DacÄƒ stringul conÈ›ine "Source: filename", extragem filename-ul
    if "Source:" in metadata_string:
        try:
            source_part = metadata_string.split("Source:")[1].strip()
            metadata_dict["document_name"] = source_part
            metadata_dict["source"] = source_part
        except:
            pass
    
    return metadata_dict

# FuncÈ›ii helper pentru debugging È™i testing
def test_json_file(file_path: str) -> None:
    """
    TesteazÄƒ complet un fiÈ™ier JSON cu structura EXACTÄ‚ È™i afiÈ™eazÄƒ rezultatele
    """
    print(f"\nğŸ” Testare fiÈ™ier: {file_path}")
    print("=" * 50)
    
    # Test validare
    is_valid, error_msg, chunks_count = validate_json_format(file_path)
    print(f"âœ… Validare: {'VALID' if is_valid else 'INVALID'}")
    if not is_valid:
        print(f"âŒ Eroare: {error_msg}")
        return
    
    # Test statistici
    stats = get_json_statistics(file_path)
    if 'error' not in stats:
        print(f"ğŸ“Š Statistici:")
        print(f"   - Total chunk-uri: {stats['total_chunks']}")
        print(f"   - Chunk-uri valide: {stats['valid_chunks']}")
        print(f"   - Dimensiune fiÈ™ier: {stats['file_size_mb']} MB")
        print(f"   - Total cuvinte: {stats['total_words']}")
        print(f"   - Surse metadata unice: {stats['unique_metadata_sources']}")
    
    # Test previzualizare
    preview = preview_json_chunks(file_path, 2)
    if 'error' not in preview:
        print(f"ğŸ‘ï¸ Previzualizare (primele 2 chunk-uri):")
        for chunk in preview['preview_chunks']:
            print(f"   - {chunk['chunk_id']}: {chunk['content_length']} caractere")
            print(f"     Metadata: {chunk['metadata']}")
    
    # Test procesare
    try:
        chunks_data = process_json_chunks(file_path)
        print(f"âœ… Procesare: {len(chunks_data)} chunk-uri procesate cu succes")
    except Exception as e:
        print(f"âŒ Eroare la procesare: {str(e)}")
    
    print("=" * 50)