def process_json_chunks(file_path: str) -> List[Dict[str, Any]]:
    """
    ProceseazÄƒ fiÈ™ierul JSON cu Ã®mbunÄƒtÄƒÈ›iri pentru cÄƒutare È™i indexare optimizatÄƒ.
    
    Returns:
        Lista de dicÈ›ionare cu chunk-uri procesate È™i Ã®mbunÄƒtÄƒÈ›ite
    """
    try:
        # ValidÄƒm mai Ã®ntÃ¢i formatul
        is_valid, error_msg, chunks_count = validate_json_format(file_path)
        if not is_valid:
            raise ValueError(f"FiÈ™ier JSON invalid: {error_msg}")
        
        # ÃncÄƒrcÄƒm datele
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Extragem chunk-urile cu procesare Ã®mbunÄƒtÄƒÈ›itÄƒ
        chunks_data = []
        chunk_pattern = re.compile(r'^chunk_(\d+)"""
Utilitare OPTIMIZATE pentru procesarea fiÈ™ierelor JSON chunkizate
Versiunea 3.0.0 - ÃmbunÄƒtÄƒÈ›iri pentru cÄƒutare È™i procesare
"""

import json
import os
import logging
import re
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
from pathlib import Path
from collections import Counter
import hashlib

logger = logging.getLogger(__name__)

def validate_json_format(file_path: str) -> Tuple[bool, str, int]:
    """
    ValideazÄƒ cÄƒ fiÈ™ierul JSON are formatul EXACT specificat cu verificÄƒri Ã®mbunÄƒtÄƒÈ›ite.
    
    Formatul acceptat:
    {
        "chunk_0": {
            "metadata": "string",
            "chunk": "content"
        }
    }
    
    Returns:
        (is_valid, error_message, chunks_count)
    """
    try:
        if not os.path.exists(file_path):
            return False, "FiÈ™ierul nu existÄƒ", 0
        
        file_size = os.path.getsize(file_path)
        if file_size == 0:
            return False, "FiÈ™ierul este gol", 0
        
        if file_size > 100 * 1024 * 1024:  # 100MB limit
            return False, f"FiÈ™ierul este prea mare ({file_size // 1024 // 1024}MB). Maximum 100MB.", 0
        
        # ÃncercÄƒm sÄƒ Ã®ncÄƒrcÄƒm JSON-ul
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError as e:
                return False, f"JSON invalid: {str(e)[:100]}...", 0
        
        # VerificÄƒm cÄƒ este un dicÈ›ionar
        if not isinstance(data, dict):
            return False, "JSON-ul trebuie sÄƒ fie un obiect (dicÈ›ionar), nu o listÄƒ sau alt tip", 0
        
        if len(data) == 0:
            return False, "JSON-ul nu conÈ›ine date", 0
        
        # CÄƒutÄƒm chunk-uri Ã®n formatul chunk_X
        chunk_pattern = re.compile(r'^chunk_\d+$')
        chunk_keys = [key for key in data.keys() if chunk_pattern.match(key)]
        
        if len(chunk_keys) == 0:
            # ÃncercÄƒm sÄƒ gÄƒsim alte patterns comune
            other_keys = list(data.keys())[:10]  # Primele 10 chei pentru debugging
            return False, f"Nu s-au gÄƒsit chunk-uri Ã®n formatul aÈ™teptat (chunk_0, chunk_1, etc.). Chei gÄƒsite: {other_keys}", 0
        
        # ValidÄƒm structura chunk-urilor - verificÄƒm mai multe pentru siguranÈ›Äƒ
        valid_chunks = 0
        invalid_chunks = []
        sample_content_lengths = []
        
        for key in chunk_keys[:min(10, len(chunk_keys))]:  # VerificÄƒm pÃ¢nÄƒ la 10 chunk-uri
            chunk = data[key]
            
            # VerificÄƒm cÄƒ chunk-ul este un dicÈ›ionar
            if not isinstance(chunk, dict):
                invalid_chunks.append(f"{key}: nu este dicÈ›ionar")
                continue
            
            # VerificÄƒm cÄƒ are EXACT cheile: "metadata" È™i "chunk"
            required_keys = {"metadata", "chunk"}
            chunk_keys_set = set(chunk.keys())
            
            if not required_keys.issubset(chunk_keys_set):
                missing_keys = required_keys - chunk_keys_set
                invalid_chunks.append(f"{key}: lipsesc cheile {missing_keys}")
                continue
            
            # VerificÄƒm cÄƒ metadata este STRING
            if not isinstance(chunk["metadata"], str):
                invalid_chunks.append(f"{key}: metadata nu este string (este {type(chunk['metadata']).__name__})")
                continue
            
            if len(chunk["metadata"].strip()) == 0:
                invalid_chunks.append(f"{key}: metadata este string gol")
                continue
            
            # VerificÄƒm cÄƒ chunk este STRING cu conÈ›inut suficient
            if not isinstance(chunk["chunk"], str):
                invalid_chunks.append(f"{key}: chunk nu este string (este {type(chunk['chunk']).__name__})")
                continue
            
            content = chunk["chunk"].strip()
            if len(content) < 10:
                invalid_chunks.append(f"{key}: conÈ›inut prea scurt ({len(content)} caractere)")
                continue
            
            sample_content_lengths.append(len(content))
            valid_chunks += 1
        
        if valid_chunks == 0:
            error_details = "; ".join(invalid_chunks[:5])  # Primele 5 erori
            return False, f"Nu s-au gÄƒsit chunk-uri valide. Erori: {error_details}", 0
        
        # VerificÄƒm consistenÈ›a numerotÄƒrii chunk-urilor
        chunk_numbers = []
        for key in chunk_keys:
            match = re.match(r'chunk_(\d+)', key)
            if match:
                chunk_numbers.append(int(match.group(1)))
        
        chunk_numbers.sort()
        expected_sequence = list(range(len(chunk_numbers)))
        
        # Warning pentru numerotare inconsistentÄƒ (nu blochez validarea)
        if chunk_numbers != expected_sequence:
            logger.warning(f"Numerotarea chunk-urilor nu este consecutivÄƒ: gÄƒsite {chunk_numbers[:10]}..., aÈ™teptate {expected_sequence[:10]}...")
        
        # Statistici pentru logging
        avg_length = sum(sample_content_lengths) / len(sample_content_lengths) if sample_content_lengths else 0
        
        logger.info(f"âœ… JSON valid gÄƒsit cu {len(chunk_keys)} chunk-uri")
        logger.info(f"ğŸ“Š Validare: {valid_chunks}/{min(10, len(chunk_keys))} chunk-uri verificate cu succes")
        logger.info(f"ğŸ“ Lungime medie conÈ›inut: {avg_length:.0f} caractere")
        
        return True, f"Valid - {len(chunk_keys)} chunk-uri gÄƒsite", len(chunk_keys)
        
    except Exception as e:
        logger.error(f"Eroare la validarea JSON: {str(e)}")
        return False, f"Eroare neaÈ™teptatÄƒ la validare: {str(e)}", 0

def process_json_chunks(file_path: str) -> List[Dict[str, Any]]:
    """
    ProceseazÄƒ fiÈ™ierul JSON cu Ã®mbunÄƒtÄƒÈ›iri pentru cÄƒutare È™i indexare optimizatÄƒ.
    
    Returns:
        Lista de dicÈ›ionare cu chunk-uri procesate È™i Ã®mbunÄƒtÄƒÈ›ite
    """
    try:
        # ValidÄƒm mai Ã®ntÃ¢i formatul
        is_vali)
        chunk_keys = [key for key in data.keys() if chunk_pattern.match(key)]
        
        # SortÄƒm chunk-urile dupÄƒ numÄƒrul lor
        def extract_chunk_number(key):
            match = chunk_pattern.match(key)
            return int(match.group(1)) if match else 0
        
        chunk_keys.sort(key=extract_chunk_number)
        
        logger.info(f"ğŸ”„ Procesare {len(chunk_keys)} chunk-uri cu optimizÄƒri...")
        
        for i, key in enumerate(chunk_keys):
            chunk = data[key]
            
            # VerificÄƒm structura EXACTÄ‚
            if not isinstance(chunk, dict) or "metadata" not in chunk or "chunk" not in chunk:
                logger.warning(f"Chunk invalid sÄƒrit: {key}")
                continue
            
            content = chunk["chunk"]
            metadata_string = chunk["metadata"]
            
            # VerificÄƒm cÄƒ sunt string-uri valide
            if not isinstance(content, str) or not isinstance(metadata_string, str):
                logger.warning(f"Chunk cu tipuri invalide sÄƒrit: {key}")
                continue
            
            # VerificÄƒm cÄƒ avem conÈ›inut suficient
            if len(content.strip()) < 10:
                logger.warning(f"Chunk cu conÈ›inut insuficient sÄƒrit: {key}")
                continue
            
            # ÃMBUNÄ‚TÄ‚ÈšIRI pentru procesare optimizatÄƒ
            
            # 1. CurÄƒÈ›Äƒm È™i normalizÄƒm conÈ›inutul
            cleaned_content = clean_and_normalize_content(content)
            
            # 2. Extragem keywords avansate
            keywords = extract_advanced_keywords(cleaned_content)
            
            # 3. DetectÄƒm limba (simplu)
            detected_language = detect_simple_language(cleaned_content)
            
            # 4. CalculÄƒm statistici de conÈ›inut
            content_stats = calculate_content_statistics(cleaned_content)
            
            # 5. Extragem entitÄƒÈ›i È™i concepte importante
            important_terms = extract_important_terms(cleaned_content)
            
            # 6. CreÄƒm un hash pentru deduplicare
            content_hash = create_content_hash(cleaned_content)
            
            # Metadata Ã®mbunÄƒtÄƒÈ›itÄƒ pentru ChromaDB
            enhanced_metadata = {
                # Metadata de bazÄƒ
                'chunk_id': key,
                'chunk_index': i,
                'chunk_number': extract_chunk_number(key),
                'file_source': os.path.basename(file_path),
                'original_source': metadata_string,
                'source': metadata_string,  # Pentru compatibilitate
                'processed_at': datetime.now().isoformat(),
                'processing_version': "3.0.0",
                
                # Statistici de conÈ›inut
                'content_length': len(cleaned_content),
                'word_count': content_stats['word_count'],
                'sentence_count': content_stats['sentence_count'],
                'paragraph_count': content_stats['paragraph_count'],
                'avg_word_length': content_stats['avg_word_length'],
                'reading_complexity': content_stats['reading_complexity'],
                
                # Keywords È™i termeni importanÈ›i
                'keywords': ", ".join(keywords[:15]),  # Top 15 keywords
                'keywords_count': len(keywords),
                'important_terms': ", ".join(important_terms[:10]),  # Top 10 termeni importanÈ›i
                'important_terms_count': len(important_terms),
                
                # Metadata pentru cÄƒutare
                'language_detected': detected_language,
                'content_type': 'json_chunk',
                'content_hash': content_hash,
                
                # Metadata pentru ranking
                'has_technical_terms': any(term in cleaned_content.lower() for term in 
                    ['function', 'method', 'class', 'variable', 'parameter', 'return', 'import', 'export']),
                'has_code_snippets': bool(re.search(r'```|`.*`|\{.*\}|\(.*\)', cleaned_content)),
                'has_numbered_lists': bool(re.search(r'\d+\.\s', cleaned_content)),
                'has_bullet_points': bool(re.search(r'[â€¢\-\*]\s', cleaned_content)),
                
                # Scoring hints pentru cÄƒutare
                'content_density': len(cleaned_content.split()) / max(1, cleaned_content.count('.') + 1),  # Cuvinte per propoziÈ›ie
                'information_richness': len(set(keywords)) / max(1, len(keywords)),  # Diversitatea keywords
                'structural_elements': content_stats['structural_elements'],
            }
            
            chunks_data.append({
                'content': cleaned_content,
                'metadata': enhanced_metadata,
                'original_content': content,  # PÄƒstrÄƒm È™i originalul pentru debugging
                'quality_score': calculate_chunk_quality_score(cleaned_content, enhanced_metadata)
            })
        
        if not chunks_data:
            raise ValueError("Nu s-au putut extrage chunk-uri valide din fiÈ™ier")
        
        # Statistici finale
        total_words = sum(chunk['metadata']['word_count'] for chunk in chunks_data)
        avg_quality = sum(chunk['quality_score'] for chunk in chunks_data) / len(chunks_data)
        
        logger.info(f"âœ… Procesat cu succes: {len(chunks_data)} chunk-uri din {file_path}")
        logger.info(f"ğŸ“Š Statistici: {total_words} cuvinte total, calitate medie: {avg_quality:.2f}")
        
        return chunks_data
        
    except Exception as e:
        logger.error(f"Eroare la procesarea fiÈ™ierului {file_path}: {str(e)}")
        raise ValueError(f"Eroare neaÈ™teptatÄƒ la procesarea fiÈ™ierului: {str(e)}")

def clean_and_normalize_content(content: str) -> str:
    """
    CurÄƒÈ›Äƒ È™i normalizeazÄƒ conÈ›inutul pentru indexare optimizatÄƒ.
    """
    if not isinstance(content, str):
        return ""
    
    # 1. EliminÄƒm comentariile HTML/Markdown
    content = re.sub(r'<!--.*?-->', '', content, flags=re.DOTALL)
    content = re.sub(r'<!-- image -->', '', content)
    
    # 2. NormalizÄƒm spaÈ›iile È™i line breaks
    content = re.sub(r'\s+', ' ', content)
    content = re.sub(r'\n\s*\n', '\n\n', content)  # PÄƒstrÄƒm paragrafele
    
    # 3. EliminÄƒm caracterele de control È™i caractere speciale problematice
    content = ''.join(char for char in content if ord(char) >= 32 or char in '\n\t')
    
    # 4. NormalizÄƒm punctuaÈ›ia
    content = re.sub(r'[""''`]', '"', content)  # UniformizÄƒm ghilimelele
    content = re.sub(r'[â€“â€”]', '-', content)     # UniformizÄƒm liniuÈ›ele
    
    # 5. EliminÄƒm liniile care sunt doar punctuaÈ›ie sau numere
    lines = content.split('\n')
    cleaned_lines = []
    for line in lines:
        line = line.strip()
        if line and not re.match(r'^[^\w]*"""
Utilitare OPTIMIZATE pentru procesarea fiÈ™ierelor JSON chunkizate
Versiunea 3.0.0 - ÃmbunÄƒtÄƒÈ›iri pentru cÄƒutare È™i procesare
"""

import json
import os
import logging
import re
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
from pathlib import Path
from collections import Counter
import hashlib

logger = logging.getLogger(__name__)

def validate_json_format(file_path: str) -> Tuple[bool, str, int]:
    """
    ValideazÄƒ cÄƒ fiÈ™ierul JSON are formatul EXACT specificat cu verificÄƒri Ã®mbunÄƒtÄƒÈ›ite.
    
    Formatul acceptat:
    {
        "chunk_0": {
            "metadata": "string",
            "chunk": "content"
        }
    }
    
    Returns:
        (is_valid, error_message, chunks_count)
    """
    try:
        if not os.path.exists(file_path):
            return False, "FiÈ™ierul nu existÄƒ", 0
        
        file_size = os.path.getsize(file_path)
        if file_size == 0:
            return False, "FiÈ™ierul este gol", 0
        
        if file_size > 100 * 1024 * 1024:  # 100MB limit
            return False, f"FiÈ™ierul este prea mare ({file_size // 1024 // 1024}MB). Maximum 100MB.", 0
        
        # ÃncercÄƒm sÄƒ Ã®ncÄƒrcÄƒm JSON-ul
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError as e:
                return False, f"JSON invalid: {str(e)[:100]}...", 0
        
        # VerificÄƒm cÄƒ este un dicÈ›ionar
        if not isinstance(data, dict):
            return False, "JSON-ul trebuie sÄƒ fie un obiect (dicÈ›ionar), nu o listÄƒ sau alt tip", 0
        
        if len(data) == 0:
            return False, "JSON-ul nu conÈ›ine date", 0
        
        # CÄƒutÄƒm chunk-uri Ã®n formatul chunk_X
        chunk_pattern = re.compile(r'^chunk_\d+$')
        chunk_keys = [key for key in data.keys() if chunk_pattern.match(key)]
        
        if len(chunk_keys) == 0:
            # ÃncercÄƒm sÄƒ gÄƒsim alte patterns comune
            other_keys = list(data.keys())[:10]  # Primele 10 chei pentru debugging
            return False, f"Nu s-au gÄƒsit chunk-uri Ã®n formatul aÈ™teptat (chunk_0, chunk_1, etc.). Chei gÄƒsite: {other_keys}", 0
        
        # ValidÄƒm structura chunk-urilor - verificÄƒm mai multe pentru siguranÈ›Äƒ
        valid_chunks = 0
        invalid_chunks = []
        sample_content_lengths = []
        
        for key in chunk_keys[:min(10, len(chunk_keys))]:  # VerificÄƒm pÃ¢nÄƒ la 10 chunk-uri
            chunk = data[key]
            
            # VerificÄƒm cÄƒ chunk-ul este un dicÈ›ionar
            if not isinstance(chunk, dict):
                invalid_chunks.append(f"{key}: nu este dicÈ›ionar")
                continue
            
            # VerificÄƒm cÄƒ are EXACT cheile: "metadata" È™i "chunk"
            required_keys = {"metadata", "chunk"}
            chunk_keys_set = set(chunk.keys())
            
            if not required_keys.issubset(chunk_keys_set):
                missing_keys = required_keys - chunk_keys_set
                invalid_chunks.append(f"{key}: lipsesc cheile {missing_keys}")
                continue
            
            # VerificÄƒm cÄƒ metadata este STRING
            if not isinstance(chunk["metadata"], str):
                invalid_chunks.append(f"{key}: metadata nu este string (este {type(chunk['metadata']).__name__})")
                continue
            
            if len(chunk["metadata"].strip()) == 0:
                invalid_chunks.append(f"{key}: metadata este string gol")
                continue
            
            # VerificÄƒm cÄƒ chunk este STRING cu conÈ›inut suficient
            if not isinstance(chunk["chunk"], str):
                invalid_chunks.append(f"{key}: chunk nu este string (este {type(chunk['chunk']).__name__})")
                continue
            
            content = chunk["chunk"].strip()
            if len(content) < 10:
                invalid_chunks.append(f"{key}: conÈ›inut prea scurt ({len(content)} caractere)")
                continue
            
            sample_content_lengths.append(len(content))
            valid_chunks += 1
        
        if valid_chunks == 0:
            error_details = "; ".join(invalid_chunks[:5])  # Primele 5 erori
            return False, f"Nu s-au gÄƒsit chunk-uri valide. Erori: {error_details}", 0
        
        # VerificÄƒm consistenÈ›a numerotÄƒrii chunk-urilor
        chunk_numbers = []
        for key in chunk_keys:
            match = re.match(r'chunk_(\d+)', key)
            if match:
                chunk_numbers.append(int(match.group(1)))
        
        chunk_numbers.sort()
        expected_sequence = list(range(len(chunk_numbers)))
        
        # Warning pentru numerotare inconsistentÄƒ (nu blochez validarea)
        if chunk_numbers != expected_sequence:
            logger.warning(f"Numerotarea chunk-urilor nu este consecutivÄƒ: gÄƒsite {chunk_numbers[:10]}..., aÈ™teptate {expected_sequence[:10]}...")
        
        # Statistici pentru logging
        avg_length = sum(sample_content_lengths) / len(sample_content_lengths) if sample_content_lengths else 0
        
        logger.info(f"âœ… JSON valid gÄƒsit cu {len(chunk_keys)} chunk-uri")
        logger.info(f"ğŸ“Š Validare: {valid_chunks}/{min(10, len(chunk_keys))} chunk-uri verificate cu succes")
        logger.info(f"ğŸ“ Lungime medie conÈ›inut: {avg_length:.0f} caractere")
        
        return True, f"Valid - {len(chunk_keys)} chunk-uri gÄƒsite", len(chunk_keys)
        
    except Exception as e:
        logger.error(f"Eroare la validarea JSON: {str(e)}")
        return False, f"Eroare neaÈ™teptatÄƒ la validare: {str(e)}", 0

def process_json_chunks(file_path: str) -> List[Dict[str, Any]]:
    """
    ProceseazÄƒ fiÈ™ierul JSON cu Ã®mbunÄƒtÄƒÈ›iri pentru cÄƒutare È™i indexare optimizatÄƒ.
    
    Returns:
        Lista de dicÈ›ionare cu chunk-uri procesate È™i Ã®mbunÄƒtÄƒÈ›ite
    """
    try:
        # ValidÄƒm mai Ã®ntÃ¢i formatul
        is_vali, line) and len(line) > 3:
            cleaned_lines.append(line)
    
    content = '\n'.join(cleaned_lines)
    
    # 6. EliminÄƒm spaÈ›iile Ã®n plus
    content = content.strip()
    
    return content

def extract_advanced_keywords(content: str, max_keywords: int = 20) -> List[str]:
    """
    Extrage keywords avansate cu filtrare È™i scoring Ã®mbunÄƒtÄƒÈ›it.
    """
    if not content:
        return []
    
    # NormalizÄƒm textul
    text_normalized = re.sub(r'[^\w\s]', ' ', content.lower())
    words = text_normalized.split()
    
    # Stop words extinse Ã®n romÃ¢nÄƒ È™i englezÄƒ
    stop_words = {
        # RomÃ¢nÄƒ - extinse
        'È™i', 'Ã®n', 'la', 'de', 'cu', 'pe', 'din', 'pentru', 'este', 'sunt', 'a', 'al', 'ale',
        'cÄƒ', 'sÄƒ', 'se', 'nu', 'mai', 'dar', 'sau', 'dacÄƒ', 'cÃ¢nd', 'cum', 'unde', 'care',
        'aceastÄƒ', 'acest', 'aceasta', 'acesta', 'unei', 'unui', 'o', 'un', 'am', 'ai', 'au',
        'avea', 'fi', 'fost', 'fiind', 'va', 'vor', 'foarte', 'mult', 'puÈ›in', 'cÄƒtre', 'despre',
        'dupÄƒ', 'Ã®nainte', 'asupra', 'printre', 'Ã®ntre', 'sub', 'peste', 'aproape', 'departe',
        'aici', 'acolo', 'undeva', 'oriunde', 'nicÄƒieri', 'atunci', 'acum', 'ieri', 'mÃ¢ine',
        'Ã®ntotdeauna', 'niciodatÄƒ', 'uneori', 'adesea', 'rar', 'prima', 'primul', 'ultima',
        'ultimul', 'toate', 'toÈ›i', 'fiecare', 'oricare', 'niciunul', 'niciuna', 'alt', 'alta',
        
        # EnglezÄƒ - extinse
        'and', 'in', 'to', 'of', 'with', 'on', 'from', 'for', 'is', 'are', 'the', 'a', 'an',
        'that', 'this', 'it', 'be', 'have', 'has', 'do', 'does', 'will', 'would', 'could',
        'should', 'may', 'might', 'can', 'must', 'shall', 'at', 'by', 'up', 'as', 'if',
        'what', 'when', 'where', 'who', 'why', 'how', 'which', 'than', 'or', 'but', 'not',
        'was', 'were', 'been', 'being', 'had', 'having', 'very', 'more', 'most', 'other',
        'some', 'all', 'any', 'each', 'every', 'few', 'many', 'much', 'several', 'such',
        'first', 'last', 'next', 'previous', 'same', 'different', 'new', 'old', 'good', 'bad',
        'big', 'small', 'long', 'short', 'high', 'low', 'right', 'left', 'here', 'there',
        'now', 'then', 'today', 'yesterday', 'tomorrow', 'always', 'never', 'sometimes',
        'often', 'rarely', 'usually', 'generally', 'probably', 'maybe', 'perhaps', 'also',
        'too', 'only', 'just', 'even', 'still', 'already', 'yet', 'again', 'once', 'twice'
    }
    
    # CalculÄƒm frecvenÈ›a cuvintelor cu scoruri Ã®mbunÄƒtÄƒÈ›ite
    word_scores = Counter()
    
    for i, word in enumerate(words):
        if (len(word) > 2 and 
            word not in stop_words and 
            not word.isdigit() and
            word.isalpha()):  # Doar cuvinte cu litere
            
            # Scor de bazÄƒ pe frecvenÈ›Äƒ
            base_score = 1
            
            # Bonus pentru cuvinte mai lungi (sunt mai specifice)
            if len(word) > 6:
                base_score += 0.5
            elif len(word) > 4:
                base_score += 0.2
            
            # Bonus pentru cuvinte care apar la Ã®nceputul textului (sunt mai importante)
            if i < len(words) * 0.2:  # Primele 20%
                base_score += 0.3
            
            # Bonus pentru cuvinte tehnice/specifice
            if any(pattern in word for pattern in ['tion', 'sion', 'ment', 'ing', 'ize', 'ise']):
                base_score += 0.2
            
            # Bonus pentru cuvinte cu majuscule Ã®n textul original (pot fi nume proprii)
            if any(word.capitalize() in content for _ in range(1)):
                base_score += 0.1
            
            word_scores[word] += base_score
    
    # Extragem È™i compound terms (2-3 cuvinte consecutive)
    compound_terms = []
    for i in range(len(words) - 1):
        if i < len(words) - 2:
            # Trigram
            trigram = ' '.join(words[i:i+3])
            if (all(len(w) > 2 and w not in stop_words for w in words[i:i+3]) and
                len(trigram) > 10):
                compound_terms.append((trigram, 2.0))  # Scor mai mare pentru trigrams
        
        # Bigram
        bigram = ' '.join(words[i:i+2])
        if (all(len(w) > 2 and w not in stop_words for w in words[i:i+2]) and
            len(bigram) > 6):
            compound_terms.append((bigram, 1.5))  # Scor moderat pentru bigrams
    
    # CombinÄƒm single words È™i compound terms
    all_terms = []
    
    # AdÄƒugÄƒm single words
    for word, score in word_scores.most_common(max_keywords):
        all_terms.append((word, score))
    
    # AdÄƒugÄƒm compound terms (pÃ¢nÄƒ la 1/3 din total)
    compound_limit = max_keywords // 3
    for term, score in sorted(compound_terms, key=lambda x: x[1], reverse=True)[:compound_limit]:
        all_terms.append((term, score))
    
    # SortÄƒm dupÄƒ scor È™i returnÄƒm
    all_terms.sort(key=lambda x: x[1], reverse=True)
    keywords = [term for term, score in all_terms[:max_keywords]]
    
    return keywords

def detect_simple_language(content: str) -> str:
    """
    DetecteazÄƒ limba conÈ›inutului (simplu, pe baza unor pattern-uri).
    """
    if not content:
        return "unknown"
    
    content_lower = content.lower()
    
    # ContorizÄƒm cuvinte specifice fiecÄƒrei limbi
    romanian_indicators = [
        'È™i', 'Ã®n', 'cu', 'pentru', 'cÄƒ', 'sÄƒ', 'de', 'la', 'pe', 'din',
        'este', 'sunt', 'avea', 'acest', 'aceastÄƒ', 'aceasta', 'RomÃ¢niei'
    ]
    
    english_indicators = [
        'the', 'and', 'with', 'for', 'that', 'this', 'have', 'has', 'are', 'is',
        'to', 'of', 'in', 'on', 'at', 'by', 'from', 'will', 'would', 'could'
    ]
    
    ro_count = sum(1 for word in romanian_indicators if f' {word} ' in f' {content_lower} ')
    en_count = sum(1 for word in english_indicators if f' {word} ' in f' {content_lower} ')
    
    if ro_count > en_count and ro_count > 3:
        return "romanian"
    elif en_count > ro_count and en_count > 3:
        return "english"
    elif ro_count > 0 or en_count > 0:
        return "mixed"
    else:
        return "unknown"

def calculate_content_statistics(content: str) -> Dict[str, Any]:
    """
    CalculeazÄƒ statistici detaliate despre conÈ›inut.
    """
    if not content:
        return {
            'word_count': 0,
            'sentence_count': 0,
            'paragraph_count': 0,
            'avg_word_length': 0,
            'reading_complexity': 'low',
            'structural_elements': 0
        }
    
    # Statistici de bazÄƒ
    words = content.split()
    word_count = len(words)
    
    # CalculÄƒm propoziÈ›ii (aproximativ)
    sentences = [s.strip() for s in re.split(r'[.!?]+', content) if s.strip()]
    sentence_count = len(sentences)
    
    # CalculÄƒm paragrafe
    paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
    paragraph_count = len(paragraphs)
    
    # Lungimea medie a cuvintelor
    avg_word_length = sum(len(word) for word in words) / max(1, word_count)
    
    # Complexitatea de citire (simplificatÄƒ)
    if avg_word_length > 6 and word_count > 100:
        reading_complexity = 'high'
    elif avg_word_length > 4 and word_count > 50:
        reading_complexity = 'medium'
    else:
        reading_complexity = 'low'
    
    # Elemente structurale
    structural_elements = 0
    structural_elements += len(re.findall(r'#+\s', content))  # Headers
    structural_elements += len(re.findall(r'\d+\.\s', content))  # Numbered lists
    structural_elements += len(re.findall(r'[â€¢\-\*]\s', content))  # Bullet points
    structural_elements += len(re.findall(r'```|`.*`', content))  # Code blocks
    
    return {
        'word_count': word_count,
        'sentence_count': sentence_count,
        'paragraph_count': paragraph_count,
        'avg_word_length': round(avg_word_length, 2),
        'reading_complexity': reading_complexity,
        'structural_elements': structural_elements
    }

def extract_important_terms(content: str, max_terms: int = 15) -> List[str]:
    """
    Extrage termeni importanÈ›i (entitÄƒÈ›i, concepte tehnice, etc.).
    """
    if not content:
        return []
    
    important_terms = []
    
    # 1. Cuvinte cu majuscule (posibile nume proprii, acronime)
    capitalized_words = re.findall(r'\b[A-Z][a-zA-Z]{2,}\b', content)
    important_terms.extend(capitalized_words[:5])
    
    # 2. Acronime (2-5 litere mari)
    acronyms = re.findall(r'\b[A-Z]{2,5}\b', content)
    important_terms.extend(acronyms[:3])
    
    # 3. Termeni tehnici comuni
    technical_patterns = [
        r'\b\w*(?:tion|sion|ment|ing|ity|ness)\b',  # Sufixe tehnice
        r'\b(?:config|setup|install|process|method|function|class|object)\w*\b',  # Termeni tehnici
        r'\b\w*(?:able|ible|ful|less|ous|ive)\b',  # Adjective complexe
    ]
    
    for pattern in technical_patterns:
        matches = re.findall(pattern, content, re.IGNORECASE)
        important_terms.extend([m for m in matches if len(m) > 4][:3])
    
    # 4. Numere È™i versiuni
    versions = re.findall(r'\b\d+\.\d+(?:\.\d+)?\b', content)
    important_terms.extend(versions[:2])
    
    # 5. URLs È™i paths (fÄƒrÄƒ protocol)
    paths = re.findall(r'\b\w+[/\\]\w+(?:[/\\]\w+)*\b', content)
    important_terms.extend(paths[:2])
    
    # CurÄƒÈ›Äƒm È™i eliminÄƒm duplicatele
    important_terms = list(set([term.strip() for term in important_terms if len(term.strip()) > 2]))
    
    # SortÄƒm dupÄƒ lungime (termenii mai lungi sunt mai specifici)
    important_terms.sort(key=len, reverse=True)
    
    return important_terms[:max_terms]

def create_content_hash(content: str) -> str:
    """
    CreeazÄƒ un hash unic pentru conÈ›inut (pentru deduplicare).
    """
    if not content:
        return ""
    
    # NormalizÄƒm conÈ›inutul pentru hash consistent
    normalized = re.sub(r'\s+', ' ', content.lower().strip())
    return hashlib.md5(normalized.encode('utf-8')).hexdigest()[:16]

def calculate_chunk_quality_score(content: str, metadata: Dict[str, Any]) -> float:
    """
    CalculeazÄƒ un scor de calitate pentru chunk (0-1).
    """
    if not content:
        return 0.0
    
    score = 0.5  # Scor de bazÄƒ
    
    # Bonus pentru lungimea optimÄƒ
    content_length = len(content)
    if 200 <= content_length <= 2000:
        score += 0.2
    elif 100 <= content_length < 200 or 2000 < content_length <= 3000:
        score += 0.1
    elif content_length < 50:
        score -= 0.2
    
    # Bonus pentru diversitatea keywords
    keywords_count = metadata.get('keywords_count', 0)
    if keywords_count > 10:
        score += 0.1
    elif keywords_count > 5:
        score += 0.05
    
    # Bonus pentru complexitatea de citire
    complexity = metadata.get('reading_complexity', 'low')
    if complexity == 'medium':
        score += 0.1
    elif complexity == 'high':
        score += 0.05
    
    # Bonus pentru elemente structurale
    structural = metadata.get('structural_elements', 0)
    if structural > 3:
        score += 0.1
    elif structural > 1:
        score += 0.05
    
    # Bonus pentru termeni importanÈ›i
    important_count = metadata.get('important_terms_count', 0)
    if important_count > 5:
        score += 0.05
    
    return min(1.0, max(0.0, score))

def get_json_statistics(file_path: str) -> Dict[str, Any]:
    """
    ObÈ›ine statistici Ã®mbunÄƒtÄƒÈ›ite despre fiÈ™ierul JSON chunkizat.
    
    Returns:
        DicÈ›ionar cu statistici detaliate despre fiÈ™ier
    """
    try:
        if not os.path.exists(file_path):
            return {"error": "FiÈ™ierul nu existÄƒ"}
        
        file_size = os.path.getsize(file_path)
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, dict):
            return {"error": "JSON-ul nu este un dicÈ›ionar"}
        
        # Analiza chunk-urilor Ã®mbunÄƒtÄƒÈ›itÄƒ
        chunk_pattern = re.compile(r'^chunk_(\d+)"""
Utilitare OPTIMIZATE pentru procesarea fiÈ™ierelor JSON chunkizate
Versiunea 3.0.0 - ÃmbunÄƒtÄƒÈ›iri pentru cÄƒutare È™i procesare
"""

import json
import os
import logging
import re
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
from pathlib import Path
from collections import Counter
import hashlib

logger = logging.getLogger(__name__)

def validate_json_format(file_path: str) -> Tuple[bool, str, int]:
    """
    ValideazÄƒ cÄƒ fiÈ™ierul JSON are formatul EXACT specificat cu verificÄƒri Ã®mbunÄƒtÄƒÈ›ite.
    
    Formatul acceptat:
    {
        "chunk_0": {
            "metadata": "string",
            "chunk": "content"
        }
    }
    
    Returns:
        (is_valid, error_message, chunks_count)
    """
    try:
        if not os.path.exists(file_path):
            return False, "FiÈ™ierul nu existÄƒ", 0
        
        file_size = os.path.getsize(file_path)
        if file_size == 0:
            return False, "FiÈ™ierul este gol", 0
        
        if file_size > 100 * 1024 * 1024:  # 100MB limit
            return False, f"FiÈ™ierul este prea mare ({file_size // 1024 // 1024}MB). Maximum 100MB.", 0
        
        # ÃncercÄƒm sÄƒ Ã®ncÄƒrcÄƒm JSON-ul
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError as e:
                return False, f"JSON invalid: {str(e)[:100]}...", 0
        
        # VerificÄƒm cÄƒ este un dicÈ›ionar
        if not isinstance(data, dict):
            return False, "JSON-ul trebuie sÄƒ fie un obiect (dicÈ›ionar), nu o listÄƒ sau alt tip", 0
        
        if len(data) == 0:
            return False, "JSON-ul nu conÈ›ine date", 0
        
        # CÄƒutÄƒm chunk-uri Ã®n formatul chunk_X
        chunk_pattern = re.compile(r'^chunk_\d+$')
        chunk_keys = [key for key in data.keys() if chunk_pattern.match(key)]
        
        if len(chunk_keys) == 0:
            # ÃncercÄƒm sÄƒ gÄƒsim alte patterns comune
            other_keys = list(data.keys())[:10]  # Primele 10 chei pentru debugging
            return False, f"Nu s-au gÄƒsit chunk-uri Ã®n formatul aÈ™teptat (chunk_0, chunk_1, etc.). Chei gÄƒsite: {other_keys}", 0
        
        # ValidÄƒm structura chunk-urilor - verificÄƒm mai multe pentru siguranÈ›Äƒ
        valid_chunks = 0
        invalid_chunks = []
        sample_content_lengths = []
        
        for key in chunk_keys[:min(10, len(chunk_keys))]:  # VerificÄƒm pÃ¢nÄƒ la 10 chunk-uri
            chunk = data[key]
            
            # VerificÄƒm cÄƒ chunk-ul este un dicÈ›ionar
            if not isinstance(chunk, dict):
                invalid_chunks.append(f"{key}: nu este dicÈ›ionar")
                continue
            
            # VerificÄƒm cÄƒ are EXACT cheile: "metadata" È™i "chunk"
            required_keys = {"metadata", "chunk"}
            chunk_keys_set = set(chunk.keys())
            
            if not required_keys.issubset(chunk_keys_set):
                missing_keys = required_keys - chunk_keys_set
                invalid_chunks.append(f"{key}: lipsesc cheile {missing_keys}")
                continue
            
            # VerificÄƒm cÄƒ metadata este STRING
            if not isinstance(chunk["metadata"], str):
                invalid_chunks.append(f"{key}: metadata nu este string (este {type(chunk['metadata']).__name__})")
                continue
            
            if len(chunk["metadata"].strip()) == 0:
                invalid_chunks.append(f"{key}: metadata este string gol")
                continue
            
            # VerificÄƒm cÄƒ chunk este STRING cu conÈ›inut suficient
            if not isinstance(chunk["chunk"], str):
                invalid_chunks.append(f"{key}: chunk nu este string (este {type(chunk['chunk']).__name__})")
                continue
            
            content = chunk["chunk"].strip()
            if len(content) < 10:
                invalid_chunks.append(f"{key}: conÈ›inut prea scurt ({len(content)} caractere)")
                continue
            
            sample_content_lengths.append(len(content))
            valid_chunks += 1
        
        if valid_chunks == 0:
            error_details = "; ".join(invalid_chunks[:5])  # Primele 5 erori
            return False, f"Nu s-au gÄƒsit chunk-uri valide. Erori: {error_details}", 0
        
        # VerificÄƒm consistenÈ›a numerotÄƒrii chunk-urilor
        chunk_numbers = []
        for key in chunk_keys:
            match = re.match(r'chunk_(\d+)', key)
            if match:
                chunk_numbers.append(int(match.group(1)))
        
        chunk_numbers.sort()
        expected_sequence = list(range(len(chunk_numbers)))
        
        # Warning pentru numerotare inconsistentÄƒ (nu blochez validarea)
        if chunk_numbers != expected_sequence:
            logger.warning(f"Numerotarea chunk-urilor nu este consecutivÄƒ: gÄƒsite {chunk_numbers[:10]}..., aÈ™teptate {expected_sequence[:10]}...")
        
        # Statistici pentru logging
        avg_length = sum(sample_content_lengths) / len(sample_content_lengths) if sample_content_lengths else 0
        
        logger.info(f"âœ… JSON valid gÄƒsit cu {len(chunk_keys)} chunk-uri")
        logger.info(f"ğŸ“Š Validare: {valid_chunks}/{min(10, len(chunk_keys))} chunk-uri verificate cu succes")
        logger.info(f"ğŸ“ Lungime medie conÈ›inut: {avg_length:.0f} caractere")
        
        return True, f"Valid - {len(chunk_keys)} chunk-uri gÄƒsite", len(chunk_keys)
        
    except Exception as e:
        logger.error(f"Eroare la validarea JSON: {str(e)}")
        return False, f"Eroare neaÈ™teptatÄƒ la validare: {str(e)}", 0

def process_json_chunks(file_path: str) -> List[Dict[str, Any]]:
    """
    ProceseazÄƒ fiÈ™ierul JSON cu Ã®mbunÄƒtÄƒÈ›iri pentru cÄƒutare È™i indexare optimizatÄƒ.
    
    Returns:
        Lista de dicÈ›ionare cu chunk-uri procesate È™i Ã®mbunÄƒtÄƒÈ›ite
    """
    try:
        # ValidÄƒm mai Ã®ntÃ¢i formatul
        is_vali)
        chunk_keys = [key for key in data.keys() if chunk_pattern.match(key)]
        
        if not chunk_keys:
            return {"error": "Nu s-au gÄƒsit chunk-uri"}
        
        # Statistici detaliate
        total_content_length = 0
        total_words = 0
        valid_chunks = 0
        metadata_sources = set()
        content_samples = []
        quality_scores = []
        language_distribution = {'romanian': 0, 'english': 0, 'mixed': 0, 'unknown': 0}
        
        for key in chunk_keys:
            chunk = data[key]
            if (isinstance(chunk, dict) and 
                "metadata" in chunk and 
                "chunk" in chunk and 
                isinstance(chunk["metadata"], str) and 
                isinstance(chunk["chunk"], str)):
                
                content = chunk["chunk"]
                metadata_string = chunk["metadata"]
                
                if len(content.strip()) >= 10:
                    valid_chunks += 1
                    
                    # ProcesÄƒm conÈ›inutul
                    cleaned_content = clean_and_normalize_content(content)
                    content_stats = calculate_content_statistics(cleaned_content)
                    
                    total_content_length += len(cleaned_content)
                    total_words += content_stats['word_count']
                    metadata_sources.add(metadata_string)
                    
                    # Sample pentru preview
                    if len(content_samples) < 3:
                        content_samples.append({
                            'chunk_id': key,
                            'preview': cleaned_content[:200] + "..." if len(cleaned_content) > 200 else cleaned_content,
                            'word_count': content_stats['word_count'],
                            'complexity': content_stats['reading_complexity']
                        })
                    
                    # DetectÄƒm limba
                    language = detect_simple_language(cleaned_content)
                    if language in language_distribution:
                        language_distribution[language] += 1
                    
                    # CalculÄƒm calitatea
                    metadata_enhanced = {
                        'keywords_count': len(extract_advanced_keywords(cleaned_content, 10)),
                        'reading_complexity': content_stats['reading_complexity'],
                        'structural_elements': content_stats['structural_elements'],
                        'important_terms_count': len(extract_important_terms(cleaned_content, 10))
                    }
                    quality_score = calculate_chunk_quality_score(cleaned_content, metadata_enhanced)
                    quality_scores.append(quality_score)
        
        # CalculÄƒm statistici finale
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        dominant_language = max(language_distribution.items(), key=lambda x: x[1])[0] if any(language_distribution.values()) else 'unknown'
        
        stats = {
            'file_info': {
                'file_name': os.path.basename(file_path),
                'file_size_bytes': file_size,
                'file_size_mb': round(file_size / (1024 * 1024), 2),
                'processed_at': datetime.now().isoformat()
            },
            'chunk_analysis': {
                'total_chunks': len(chunk_keys),
                'valid_chunks': valid_chunks,
                'invalid_chunks': len(chunk_keys) - valid_chunks,
                'chunk_numbering_consistent': len(chunk_keys) == max([int(re.match(r'chunk_(\d+)', key).group(1)) for key in chunk_keys]) + 1 if chunk_keys else True
            },
            'content_statistics': {
                'total_content_length': total_content_length,
                'total_words': total_words,
                'average_chunk_length': round(total_content_length / valid_chunks) if valid_chunks > 0 else 0,
                'average_words_per_chunk': round(total_words / valid_chunks) if valid_chunks > 0 else 0,
                'average_quality_score': round(avg_quality, 3)
            },
            'metadata_analysis': {
                'unique_metadata_sources': len(metadata_sources),
                'metadata_sources_list': list(metadata_sources)[:10],  # Primele 10
                'dominant_language': dominant_language,
                'language_distribution': language_distribution
            },
            'content_samples': content_samples,
            'quality_distribution': {
                'high_quality': len([s for s in quality_scores if s > 0.7]),
                'medium_quality': len([s for s in quality_scores if 0.4 <= s <= 0.7]),
                'low_quality': len([s for s in quality_scores if s < 0.4]),
            },
            'processing_recommendations': generate_processing_recommendations(
                valid_chunks, avg_quality, dominant_language, len(metadata_sources)
            )
        }
        
        logger.debug(f"ğŸ“Š Statistici Ã®mbunÄƒtÄƒÈ›ite generate pentru {file_path}: {valid_chunks} chunk-uri valide, calitate {avg_quality:.2f}")
        return stats
        
    except Exception as e:
        logger.error(f"Eroare la generarea statisticilor pentru {file_path}: {str(e)}")
        return {"error": f"Eroare la analiza fiÈ™ierului: {str(e)}"}

def generate_processing_recommendations(valid_chunks: int, avg_quality: float, language: str, source_count: int) -> List[str]:
    """
    GenereazÄƒ recomandÄƒri pentru procesarea optimÄƒ a fiÈ™ierului.
    """
    recommendations = []
    
    if valid_chunks < 5:
        recommendations.append("FiÈ™ierul conÈ›ine puÈ›ine chunk-uri. ConsideraÈ›i combinarea cu alte fiÈ™iere.")
    elif valid_chunks > 1000:
        recommendations.append("FiÈ™ierul este foarte mare. ConsideraÈ›i Ã®mpÄƒrÈ›irea Ã®n fiÈ™iere mai mici.")
    
    if avg_quality < 0.5:
        recommendations.append("Calitatea chunk-urilor este scÄƒzutÄƒ. VerificaÈ›i formatarea È™i conÈ›inutul.")
    elif avg_quality > 0.8:
        recommendations.append("Chunk-urile au calitate excelentÄƒ. Ideal pentru indexare È™i cÄƒutare.")
    
    if language == 'mixed':
        recommendations.append("ConÈ›inutul este Ã®n limbi mixte. CÄƒutarea hibridÄƒ va fi foarte utilÄƒ.")
    elif language == 'unknown':
        recommendations.append("Limba nu a putut fi detectatÄƒ. VerificaÈ›i codificarea È™i conÈ›inutul textului.")
    
    if source_count == 1:
        recommendations.append("Toate chunk-urile provin din aceeaÈ™i sursÄƒ. DiversificaÈ›i sursele pentru rezultate mai bune.")
    elif source_count > 10:
        recommendations.append("Multe surse diferite detectate. Excelent pentru diversitatea conÈ›inutului.")
    
    if not recommendations:
        recommendations.append("FiÈ™ierul este optimizat pentru procesare. Nu sunt necesare ajustÄƒri.")
    
    return recommendations

def preview_json_chunks(file_path: str, max_chunks: int = 3) -> Dict[str, Any]:
    """
    OferÄƒ o previzualizare Ã®mbunÄƒtÄƒÈ›itÄƒ a chunk-urilor din fiÈ™ier.
    
    Args:
        file_path: Calea cÄƒtre fiÈ™ierul JSON
        max_chunks: NumÄƒrul maxim de chunk-uri de previzualizat
        
    Returns:
        DicÈ›ionar cu previzualizarea Ã®mbunÄƒtÄƒÈ›itÄƒ a chunk-urilor
    """
    try:
        if not os.path.exists(file_path):
            return {"error": "FiÈ™ierul nu existÄƒ"}
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, dict):
            return {"error": "JSON-ul nu este un dicÈ›ionar"}
        
        chunk_pattern = re.compile(r'^chunk_(\d+)"""
Utilitare OPTIMIZATE pentru procesarea fiÈ™ierelor JSON chunkizate
Versiunea 3.0.0 - ÃmbunÄƒtÄƒÈ›iri pentru cÄƒutare È™i procesare
"""

import json
import os
import logging
import re
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
from pathlib import Path
from collections import Counter
import hashlib

logger = logging.getLogger(__name__)

def validate_json_format(file_path: str) -> Tuple[bool, str, int]:
    """
    ValideazÄƒ cÄƒ fiÈ™ierul JSON are formatul EXACT specificat cu verificÄƒri Ã®mbunÄƒtÄƒÈ›ite.
    
    Formatul acceptat:
    {
        "chunk_0": {
            "metadata": "string",
            "chunk": "content"
        }
    }
    
    Returns:
        (is_valid, error_message, chunks_count)
    """
    try:
        if not os.path.exists(file_path):
            return False, "FiÈ™ierul nu existÄƒ", 0
        
        file_size = os.path.getsize(file_path)
        if file_size == 0:
            return False, "FiÈ™ierul este gol", 0
        
        if file_size > 100 * 1024 * 1024:  # 100MB limit
            return False, f"FiÈ™ierul este prea mare ({file_size // 1024 // 1024}MB). Maximum 100MB.", 0
        
        # ÃncercÄƒm sÄƒ Ã®ncÄƒrcÄƒm JSON-ul
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError as e:
                return False, f"JSON invalid: {str(e)[:100]}...", 0
        
        # VerificÄƒm cÄƒ este un dicÈ›ionar
        if not isinstance(data, dict):
            return False, "JSON-ul trebuie sÄƒ fie un obiect (dicÈ›ionar), nu o listÄƒ sau alt tip", 0
        
        if len(data) == 0:
            return False, "JSON-ul nu conÈ›ine date", 0
        
        # CÄƒutÄƒm chunk-uri Ã®n formatul chunk_X
        chunk_pattern = re.compile(r'^chunk_\d+$')
        chunk_keys = [key for key in data.keys() if chunk_pattern.match(key)]
        
        if len(chunk_keys) == 0:
            # ÃncercÄƒm sÄƒ gÄƒsim alte patterns comune
            other_keys = list(data.keys())[:10]  # Primele 10 chei pentru debugging
            return False, f"Nu s-au gÄƒsit chunk-uri Ã®n formatul aÈ™teptat (chunk_0, chunk_1, etc.). Chei gÄƒsite: {other_keys}", 0
        
        # ValidÄƒm structura chunk-urilor - verificÄƒm mai multe pentru siguranÈ›Äƒ
        valid_chunks = 0
        invalid_chunks = []
        sample_content_lengths = []
        
        for key in chunk_keys[:min(10, len(chunk_keys))]:  # VerificÄƒm pÃ¢nÄƒ la 10 chunk-uri
            chunk = data[key]
            
            # VerificÄƒm cÄƒ chunk-ul este un dicÈ›ionar
            if not isinstance(chunk, dict):
                invalid_chunks.append(f"{key}: nu este dicÈ›ionar")
                continue
            
            # VerificÄƒm cÄƒ are EXACT cheile: "metadata" È™i "chunk"
            required_keys = {"metadata", "chunk"}
            chunk_keys_set = set(chunk.keys())
            
            if not required_keys.issubset(chunk_keys_set):
                missing_keys = required_keys - chunk_keys_set
                invalid_chunks.append(f"{key}: lipsesc cheile {missing_keys}")
                continue
            
            # VerificÄƒm cÄƒ metadata este STRING
            if not isinstance(chunk["metadata"], str):
                invalid_chunks.append(f"{key}: metadata nu este string (este {type(chunk['metadata']).__name__})")
                continue
            
            if len(chunk["metadata"].strip()) == 0:
                invalid_chunks.append(f"{key}: metadata este string gol")
                continue
            
            # VerificÄƒm cÄƒ chunk este STRING cu conÈ›inut suficient
            if not isinstance(chunk["chunk"], str):
                invalid_chunks.append(f"{key}: chunk nu este string (este {type(chunk['chunk']).__name__})")
                continue
            
            content = chunk["chunk"].strip()
            if len(content) < 10:
                invalid_chunks.append(f"{key}: conÈ›inut prea scurt ({len(content)} caractere)")
                continue
            
            sample_content_lengths.append(len(content))
            valid_chunks += 1
        
        if valid_chunks == 0:
            error_details = "; ".join(invalid_chunks[:5])  # Primele 5 erori
            return False, f"Nu s-au gÄƒsit chunk-uri valide. Erori: {error_details}", 0
        
        # VerificÄƒm consistenÈ›a numerotÄƒrii chunk-urilor
        chunk_numbers = []
        for key in chunk_keys:
            match = re.match(r'chunk_(\d+)', key)
            if match:
                chunk_numbers.append(int(match.group(1)))
        
        chunk_numbers.sort()
        expected_sequence = list(range(len(chunk_numbers)))
        
        # Warning pentru numerotare inconsistentÄƒ (nu blochez validarea)
        if chunk_numbers != expected_sequence:
            logger.warning(f"Numerotarea chunk-urilor nu este consecutivÄƒ: gÄƒsite {chunk_numbers[:10]}..., aÈ™teptate {expected_sequence[:10]}...")
        
        # Statistici pentru logging
        avg_length = sum(sample_content_lengths) / len(sample_content_lengths) if sample_content_lengths else 0
        
        logger.info(f"âœ… JSON valid gÄƒsit cu {len(chunk_keys)} chunk-uri")
        logger.info(f"ğŸ“Š Validare: {valid_chunks}/{min(10, len(chunk_keys))} chunk-uri verificate cu succes")
        logger.info(f"ğŸ“ Lungime medie conÈ›inut: {avg_length:.0f} caractere")
        
        return True, f"Valid - {len(chunk_keys)} chunk-uri gÄƒsite", len(chunk_keys)
        
    except Exception as e:
        logger.error(f"Eroare la validarea JSON: {str(e)}")
        return False, f"Eroare neaÈ™teptatÄƒ la validare: {str(e)}", 0

def process_json_chunks(file_path: str) -> List[Dict[str, Any]]:
    """
    ProceseazÄƒ fiÈ™ierul JSON cu Ã®mbunÄƒtÄƒÈ›iri pentru cÄƒutare È™i indexare optimizatÄƒ.
    
    Returns:
        Lista de dicÈ›ionare cu chunk-uri procesate È™i Ã®mbunÄƒtÄƒÈ›ite
    """
    try:
        # ValidÄƒm mai Ã®ntÃ¢i formatul
        is_vali)
        chunk_keys = [key for key in data.keys() if chunk_pattern.match(key)]
        
        if not chunk_keys:
            return {"error": "Nu s-au gÄƒsit chunk-uri"}
        
        # SortÄƒm chunk-urile
        def extract_chunk_number(key):
            match = chunk_pattern.match(key)
            return int(match.group(1)) if match else 0
        
        chunk_keys.sort(key=extract_chunk_number)
        
        # SelectÄƒm chunk-urile pentru previzualizare
        preview_chunks = []
        
        for key in chunk_keys[:max_chunks]:
            chunk = data[key]
            
            if (isinstance(chunk, dict) and 
                "metadata" in chunk and 
                "chunk" in chunk and 
                isinstance(chunk["metadata"], str) and 
                isinstance(chunk["chunk"], str)):
                
                content = chunk["chunk"]
                metadata_string = chunk["metadata"]
                
                # ProcesÄƒm conÈ›inutul pentru previzualizare
                cleaned_content = clean_and_normalize_content(content)
                content_stats = calculate_content_statistics(cleaned_content)
                keywords = extract_advanced_keywords(cleaned_content, 8)
                important_terms = extract_important_terms(cleaned_content, 5)
                language = detect_simple_language(cleaned_content)
                
                # CreÄƒm preview-ul conÈ›inutului
                if len(cleaned_content) > 300:
                    preview_content = cleaned_content[:300] + "..."
                else:
                    preview_content = cleaned_content
                
                # CalculÄƒm scorul de calitate
                metadata_enhanced = {
                    'keywords_count': len(keywords),
                    'reading_complexity': content_stats['reading_complexity'],
                    'structural_elements': content_stats['structural_elements'],
                    'important_terms_count': len(important_terms)
                }
                quality_score = calculate_chunk_quality_score(cleaned_content, metadata_enhanced)
                
                preview_chunks.append({
                    'chunk_id': key,
                    'chunk_number': extract_chunk_number(key),
                    'content_preview': preview_content,
                    'content_length': len(cleaned_content),
                    'word_count': content_stats['word_count'],
                    'sentence_count': content_stats['sentence_count'],
                    'metadata': metadata_string,
                    'keywords': keywords[:5],  # Top 5 keywords
                    'important_terms': important_terms[:3],  # Top 3 termeni importanÈ›i
                    'language_detected': language,
                    'reading_complexity': content_stats['reading_complexity'],
                    'quality_score': round(quality_score, 3),
                    'has_structural_elements': content_stats['structural_elements'] > 0,
                    'processing_suggestions': generate_chunk_suggestions(cleaned_content, content_stats)
                })
        
        # CalculÄƒm statistici generale pentru previzualizare
        total_quality = sum(chunk['quality_score'] for chunk in preview_chunks)
        avg_quality = total_quality / len(preview_chunks) if preview_chunks else 0
        
        languages = [chunk['language_detected'] for chunk in preview_chunks]
        language_counts = Counter(languages)
        dominant_language = language_counts.most_common(1)[0][0] if language_counts else 'unknown'
        
        result = {
            'file_info': {
                'file_name': os.path.basename(file_path),
                'total_chunks': len(chunk_keys),
                'preview_count': len(preview_chunks),
                'showing': f"{min(len(preview_chunks), max_chunks)} din {len(chunk_keys)} chunk-uri"
            },
            'preview_chunks': preview_chunks,
            'preview_analysis': {
                'average_quality': round(avg_quality, 3),
                'dominant_language': dominant_language,
                'language_distribution': dict(language_counts),
                'total_words_preview': sum(chunk['word_count'] for chunk in preview_chunks),
                'complexity_distribution': Counter(chunk['reading_complexity'] for chunk in preview_chunks)
            },
            'recommendations': generate_preview_recommendations(preview_chunks, avg_quality, dominant_language)
        }
        
        logger.debug(f"ğŸ‘ï¸ Previzualizare Ã®mbunÄƒtÄƒÈ›itÄƒ generatÄƒ pentru {file_path}: {len(preview_chunks)} chunk-uri")
        return result
        
    except Exception as e:
        logger.error(f"Eroare la previzualizarea fiÈ™ierului {file_path}: {str(e)}")
        return {"error": f"Eroare la previzualizare: {str(e)}"}

def generate_chunk_suggestions(content: str, stats: Dict[str, Any]) -> List[str]:
    """
    GenereazÄƒ sugestii specifice pentru Ã®mbunÄƒtÄƒÈ›irea unui chunk.
    """
    suggestions = []
    
    if stats['word_count'] < 20:
        suggestions.append("Chunk foarte scurt - consideraÈ›i combinarea cu chunk-uri adiacente")
    elif stats['word_count'] > 500:
        suggestions.append("Chunk foarte lung - consideraÈ›i Ã®mpÄƒrÈ›irea Ã®n chunk-uri mai mici")
    
    if stats['sentence_count'] < 2:
        suggestions.append("AdÄƒugaÈ›i mai multe propoziÈ›ii pentru context mai bogat")
    
    if stats['reading_complexity'] == 'high':
        suggestions.append("Complexitate mare - excelent pentru conÈ›inut tehnic detaliat")
    elif stats['reading_complexity'] == 'low':
        suggestions.append("Complexitate scÄƒzutÄƒ - consideraÈ›i adÄƒugarea de detalii")
    
    if stats['structural_elements'] == 0:
        suggestions.append("AdÄƒugaÈ›i elemente structurale (liste, headers) pentru organizare mai bunÄƒ")
    
    if not suggestions:
        suggestions.append("Chunk-ul este bine structurat È™i optimizat")
    
    return suggestions

def generate_preview_recommendations(chunks: List[Dict], avg_quality: float, language: str) -> List[str]:
    """
    GenereazÄƒ recomandÄƒri bazate pe previzualizarea chunk-urilor.
    """
    recommendations = []
    
    if avg_quality < 0.4:
        recommendations.append("ğŸ”§ Calitatea chunk-urilor este scÄƒzutÄƒ. ReviziÈ›i formatarea È™i conÈ›inutul.")
    elif avg_quality > 0.8:
        recommendations.append("âœ… Chunk-urile au calitate excelentÄƒ. FiÈ™ierul este optimizat pentru cÄƒutare.")
    
    word_counts = [chunk['word_count'] for chunk in chunks]
    if word_counts:
        avg_words = sum(word_counts) / len(word_counts)
        if avg_words < 30:
            recommendations.append("ğŸ“ Chunk-urile sunt scurte. ConsideraÈ›i combinarea pentru context mai bogat.")
        elif avg_words > 300:
            recommendations.append("ğŸ“„ Chunk-urile sunt lungi. ConsideraÈ›i Ã®mpÄƒrÈ›irea pentru cÄƒutare mai precisÄƒ.")
    
    if language == 'mixed':
        recommendations.append("ğŸŒ ConÈ›inut multilingv detectat. ActivaÈ›i cÄƒutarea hibridÄƒ pentru rezultate optime.")
    elif language == 'unknown':
        recommendations.append("â“ Limba nedeterminatÄƒ. VerificaÈ›i codificarea textului.")
    
    # AnalizÄƒm diversitatea keywords
    all_keywords = []
    for chunk in chunks:
        all_keywords.extend(chunk.get('keywords', []))
    
    unique_keywords = len(set(all_keywords))
    total_keywords = len(all_keywords)
    
    if total_keywords > 0:
        diversity_ratio = unique_keywords / total_keywords
        if diversity_ratio > 0.8:
            recommendations.append("ğŸ¯ Diversitate mare de keywords. Excelent pentru cÄƒutÄƒri variate.")
        elif diversity_ratio < 0.4:
            recommendations.append("ğŸ”„ Keywords repetitive. ConsideraÈ›i diversificarea vocabularului.")
    
    if not recommendations:
        recommendations.append("ğŸ‰ FiÈ™ierul este excelent optimizat pentru sistemul RAG!")
    
    return recommendations

def validate_chunk_structure(chunk_data: Dict[str, Any]) -> Tuple[bool, str]:
    """
    ValideazÄƒ structura EXACTÄ‚ a unui chunk individual cu verificÄƒri Ã®mbunÄƒtÄƒÈ›ite.
    
    Args:
        chunk_data: Datele chunk-ului de validat
        
    Returns:
        (is_valid, error_message)
    """
    try:
        if not isinstance(chunk_data, dict):
            return False, "Chunk-ul trebuie sÄƒ fie un dicÈ›ionar"
        
        # VerificÄƒm cheile EXACTE
        required_keys = {"metadata", "chunk"}
        chunk_keys = set(chunk_data.keys())
        
        # VerificÄƒm chei lipsÄƒ
        missing_keys = required_keys - chunk_keys
        if missing_keys:
            return False, f"Lipsesc cheile obligatorii: {missing_keys}"
        
        # VerificÄƒm chei Ã®n plus (opÈ›ional - warning, nu eroare)
        extra_keys = chunk_keys - required_keys
        if extra_keys:
            logger.warning(f"Chei suplimentare gÄƒsite Ã®n chunk: {extra_keys}")
        
        # ValidÄƒm metadata - TREBUIE sÄƒ fie string
        metadata = chunk_data["metadata"]
        if not isinstance(metadata, str):
            return False, f"Metadata trebuie sÄƒ fie string, nu {type(metadata).__name__}"
        
        metadata_stripped = metadata.strip()
        if len(metadata_stripped) == 0:
            return False, "Metadata nu poate fi string gol"
        
        if len(metadata_stripped) > 1000:
            return False, f"Metadata prea lungÄƒ ({len(metadata_stripped)} caractere, maximum 1000)"
        
        # ValidÄƒm conÈ›inutul - TREBUIE sÄƒ fie string
        content = chunk_data["chunk"]
        if not isinstance(content, str):
            return False, f"ConÈ›inutul chunk-ului trebuie sÄƒ fie string, nu {type(content).__name__}"
        
        content_stripped = content.strip()
        if len(content_stripped) < 10:
            return False, f"ConÈ›inutul chunk-ului este prea scurt ({len(content_stripped)} caractere, minimum 10)"
        
        if len(content_stripped) > 50000:
            logger.warning(f"Chunk foarte lung detectat: {len(content_stripped)} caractere")
        
        # VerificÄƒri de calitate suplimentare
        word_count = len(content_stripped.split())
        if word_count < 3:
            return False, f"Chunk-ul conÈ›ine prea puÈ›ine cuvinte ({word_count}, minimum 3)"
        
        # VerificÄƒm dacÄƒ conÈ›inutul nu este doar punctuaÈ›ie
        alphanumeric_chars = sum(1 for c in content_stripped if c.isalnum())
        if alphanumeric_chars < 5:
            return False, "Chunk-ul trebuie sÄƒ conÈ›inÄƒ cel puÈ›in 5 caractere alfanumerice"
        
        return True, "Chunk valid cu structura exactÄƒ È™i conÈ›inut de calitate"
        
    except Exception as e:
        return False, f"Eroare la validarea chunk-ului: {str(e)}"

def clean_chunk_content(content: str) -> str:
    """
    CurÄƒÈ›Äƒ È™i normalizeazÄƒ conÈ›inutul unui chunk cu algoritmi Ã®mbunÄƒtÄƒÈ›iÈ›i.
    
    Args:
        content: ConÈ›inutul de curÄƒÈ›at
        
    Returns:
        ConÈ›inutul curÄƒÈ›at È™i optimizat
    """
    if not isinstance(content, str):
        return ""
    
    # 1. EliminÄƒm spaÈ›iile Ã®n plus de la Ã®nceput È™i sfÃ¢rÈ™it
    content = content.strip()
    
    if not content:
        return ""
    
    # 2. NormalizÄƒm line endings
    content = content.replace('\r\n', '\n').replace('\r', '\n')
    
    # 3. EliminÄƒm comentariile HTML È™i Markdown
    content = re.sub(r'<!--.*?-->', '', content, flags=re.DOTALL)
    content = re.sub(r'<!-- image -->', '', content)
    
    # 4. NormalizÄƒm spaÈ›iile multiple, dar pÄƒstrÄƒm structura paragrafelor
    content = re.sub(r'[ \t]+', ' ', content)  # SpaÈ›ii È™i tab-uri multiple -> un spaÈ›iu
    content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)  # Line breaks multiple -> douÄƒ line breaks
    
    # 5. EliminÄƒm caracterele de control, dar pÄƒstrÄƒm \n È™i \t
    content = ''.join(char for char in content if ord(char) >= 32 or char in '\n\t')
    
    # 6. NormalizÄƒm punctuaÈ›ia pentru consistenÈ›Äƒ
    content = re.sub(r'[""''`]', '"', content)  # UniformizÄƒm ghilimelele
    content = re.sub(r'[â€“â€”]', '-', content)     # UniformizÄƒm liniuÈ›ele
    content = re.sub(r'â€¦', '...', content)      # UniformizÄƒm ellipsis
    
    # 7. EliminÄƒm liniile care sunt doar punctuaÈ›ie sau whitespace
    lines = content.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line_stripped = line.strip()
        # PÄƒstrÄƒm linia dacÄƒ:
        # - Nu este goalÄƒ
        # - Nu este doar punctuaÈ›ie
        # - Are cel puÈ›in 2 caractere alfanumerice
        if (line_stripped and 
            not re.match(r'^[^\w]*"""
Utilitare OPTIMIZATE pentru procesarea fiÈ™ierelor JSON chunkizate
Versiunea 3.0.0 - ÃmbunÄƒtÄƒÈ›iri pentru cÄƒutare È™i procesare
"""

import json
import os
import logging
import re
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
from pathlib import Path
from collections import Counter
import hashlib

logger = logging.getLogger(__name__)

def validate_json_format(file_path: str) -> Tuple[bool, str, int]:
    """
    ValideazÄƒ cÄƒ fiÈ™ierul JSON are formatul EXACT specificat cu verificÄƒri Ã®mbunÄƒtÄƒÈ›ite.
    
    Formatul acceptat:
    {
        "chunk_0": {
            "metadata": "string",
            "chunk": "content"
        }
    }
    
    Returns:
        (is_valid, error_message, chunks_count)
    """
    try:
        if not os.path.exists(file_path):
            return False, "FiÈ™ierul nu existÄƒ", 0
        
        file_size = os.path.getsize(file_path)
        if file_size == 0:
            return False, "FiÈ™ierul este gol", 0
        
        if file_size > 100 * 1024 * 1024:  # 100MB limit
            return False, f"FiÈ™ierul este prea mare ({file_size // 1024 // 1024}MB). Maximum 100MB.", 0
        
        # ÃncercÄƒm sÄƒ Ã®ncÄƒrcÄƒm JSON-ul
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError as e:
                return False, f"JSON invalid: {str(e)[:100]}...", 0
        
        # VerificÄƒm cÄƒ este un dicÈ›ionar
        if not isinstance(data, dict):
            return False, "JSON-ul trebuie sÄƒ fie un obiect (dicÈ›ionar), nu o listÄƒ sau alt tip", 0
        
        if len(data) == 0:
            return False, "JSON-ul nu conÈ›ine date", 0
        
        # CÄƒutÄƒm chunk-uri Ã®n formatul chunk_X
        chunk_pattern = re.compile(r'^chunk_\d+$')
        chunk_keys = [key for key in data.keys() if chunk_pattern.match(key)]
        
        if len(chunk_keys) == 0:
            # ÃncercÄƒm sÄƒ gÄƒsim alte patterns comune
            other_keys = list(data.keys())[:10]  # Primele 10 chei pentru debugging
            return False, f"Nu s-au gÄƒsit chunk-uri Ã®n formatul aÈ™teptat (chunk_0, chunk_1, etc.). Chei gÄƒsite: {other_keys}", 0
        
        # ValidÄƒm structura chunk-urilor - verificÄƒm mai multe pentru siguranÈ›Äƒ
        valid_chunks = 0
        invalid_chunks = []
        sample_content_lengths = []
        
        for key in chunk_keys[:min(10, len(chunk_keys))]:  # VerificÄƒm pÃ¢nÄƒ la 10 chunk-uri
            chunk = data[key]
            
            # VerificÄƒm cÄƒ chunk-ul este un dicÈ›ionar
            if not isinstance(chunk, dict):
                invalid_chunks.append(f"{key}: nu este dicÈ›ionar")
                continue
            
            # VerificÄƒm cÄƒ are EXACT cheile: "metadata" È™i "chunk"
            required_keys = {"metadata", "chunk"}
            chunk_keys_set = set(chunk.keys())
            
            if not required_keys.issubset(chunk_keys_set):
                missing_keys = required_keys - chunk_keys_set
                invalid_chunks.append(f"{key}: lipsesc cheile {missing_keys}")
                continue
            
            # VerificÄƒm cÄƒ metadata este STRING
            if not isinstance(chunk["metadata"], str):
                invalid_chunks.append(f"{key}: metadata nu este string (este {type(chunk['metadata']).__name__})")
                continue
            
            if len(chunk["metadata"].strip()) == 0:
                invalid_chunks.append(f"{key}: metadata este string gol")
                continue
            
            # VerificÄƒm cÄƒ chunk este STRING cu conÈ›inut suficient
            if not isinstance(chunk["chunk"], str):
                invalid_chunks.append(f"{key}: chunk nu este string (este {type(chunk['chunk']).__name__})")
                continue
            
            content = chunk["chunk"].strip()
            if len(content) < 10:
                invalid_chunks.append(f"{key}: conÈ›inut prea scurt ({len(content)} caractere)")
                continue
            
            sample_content_lengths.append(len(content))
            valid_chunks += 1
        
        if valid_chunks == 0:
            error_details = "; ".join(invalid_chunks[:5])  # Primele 5 erori
            return False, f"Nu s-au gÄƒsit chunk-uri valide. Erori: {error_details}", 0
        
        # VerificÄƒm consistenÈ›a numerotÄƒrii chunk-urilor
        chunk_numbers = []
        for key in chunk_keys:
            match = re.match(r'chunk_(\d+)', key)
            if match:
                chunk_numbers.append(int(match.group(1)))
        
        chunk_numbers.sort()
        expected_sequence = list(range(len(chunk_numbers)))
        
        # Warning pentru numerotare inconsistentÄƒ (nu blochez validarea)
        if chunk_numbers != expected_sequence:
            logger.warning(f"Numerotarea chunk-urilor nu este consecutivÄƒ: gÄƒsite {chunk_numbers[:10]}..., aÈ™teptate {expected_sequence[:10]}...")
        
        # Statistici pentru logging
        avg_length = sum(sample_content_lengths) / len(sample_content_lengths) if sample_content_lengths else 0
        
        logger.info(f"âœ… JSON valid gÄƒsit cu {len(chunk_keys)} chunk-uri")
        logger.info(f"ğŸ“Š Validare: {valid_chunks}/{min(10, len(chunk_keys))} chunk-uri verificate cu succes")
        logger.info(f"ğŸ“ Lungime medie conÈ›inut: {avg_length:.0f} caractere")
        
        return True, f"Valid - {len(chunk_keys)} chunk-uri gÄƒsite", len(chunk_keys)
        
    except Exception as e:
        logger.error(f"Eroare la validarea JSON: {str(e)}")
        return False, f"Eroare neaÈ™teptatÄƒ la validare: {str(e)}", 0

def process_json_chunks(file_path: str) -> List[Dict[str, Any]]:
    """
    ProceseazÄƒ fiÈ™ierul JSON cu Ã®mbunÄƒtÄƒÈ›iri pentru cÄƒutare È™i indexare optimizatÄƒ.
    
    Returns:
        Lista de dicÈ›ionare cu chunk-uri procesate È™i Ã®mbunÄƒtÄƒÈ›ite
    """
    try:
        # ValidÄƒm mai Ã®ntÃ¢i formatul
        is_vali, line_stripped) and
            sum(1 for c in line_stripped if c.isalnum()) >= 2):
            cleaned_lines.append(line)
        elif line_stripped == '':
            # PÄƒstrÄƒm liniile goale pentru separarea paragrafelor
            cleaned_lines.append('')
    
    content = '\n'.join(cleaned_lines)
    
    # 8. EliminÄƒm paragrafele goale multiple
    content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)
    
    # 9. CurÄƒÈ›Äƒm din nou spaÈ›iile de la Ã®nceput È™i sfÃ¢rÈ™it
    content = content.strip()
    
    # 10. Verificare finalÄƒ de calitate
    if len(content) < 10:
        logger.warning(f"ConÈ›inut foarte scurt dupÄƒ curÄƒÈ›are: '{content[:50]}...'")
    
    return content

def extract_metadata_info(metadata_string: str) -> Dict[str, Any]:
    """
    Extrage informaÈ›ii Ã®mbunÄƒtÄƒÈ›ite din metadata string È™i le converteÈ™te Ã®n dict.
    
    Args:
        metadata_string: Metadata originalÄƒ ca string
        
    Returns:
        Metadata convertitÄƒ Ã®n dict cu informaÈ›ii Ã®mbunÄƒtÄƒÈ›ite
    """
    if not isinstance(metadata_string, str):
        return {"original_source": "Necunoscut", "source": "Necunoscut"}
    
    metadata_dict = {
        "original_source": metadata_string,
        "source": metadata_string,
        "extracted_info": {}
    }
    
    # Pattern matching Ã®mbunÄƒtÄƒÈ›it pentru diferite formate de metadata
    
    # 1. Format "Source: filename"
    source_match = re.search(r'Source:\s*(.+?)(?:\s*$|\s*\||\s*,)', metadata_string, re.IGNORECASE)
    if source_match:
        document_name = source_match.group(1).strip()
        metadata_dict["document_name"] = document_name
        metadata_dict["source"] = document_name
        metadata_dict["extracted_info"]["document_name"] = document_name
    
    # 2. DetectÄƒm tipul de fiÈ™ier
    file_extensions = re.findall(r'\.([a-zA-Z0-9]+)', metadata_string)
    if file_extensions:
        metadata_dict["file_type"] = file_extensions[-1].lower()  # Ultima extensie gÄƒsitÄƒ
        metadata_dict["extracted_info"]["file_extensions"] = file_extensions
    
    # 3. DetectÄƒm date
    date_patterns = [
        r'\b(\d{4}[-/]\d{1,2}[-/]\d{1,2})\b',  # YYYY-MM-DD sau YYYY/MM/DD
        r'\b(\d{1,2}[-/]\d{1,2}[-/]\d{4})\b',  # MM-DD-YYYY sau MM/DD/YYYY
        r'\b([A-Za-z]{3}\s+\d{1,2},?\s+\d{4})\b',  # Mar 15, 2024
        r'\b(\d{1,2}\s+[A-Za-z]{3,9}\s+\d{4})\b'   # 15 March 2024
    ]
    
    for pattern in date_patterns:
        dates = re.findall(pattern, metadata_string)
        if dates:
            metadata_dict["extracted_info"]["dates_found"] = dates
            metadata_dict["date_context"] = dates[0]  # Prima datÄƒ gÄƒsitÄƒ
            break
    
    # 4. DetectÄƒm autori sau nume
    author_patterns = [
        r'(?:by|author|written by|created by):\s*([A-Za-z\s]+?)(?:\s*$|\s*\||\s*,)',
        r'\b([A-Z][a-z]+\s+[A-Z][a-z]+)\b'  # Nume proprii (FirstName LastName)
    ]
    
    for pattern in author_patterns:
        authors = re.findall(pattern, metadata_string, re.IGNORECASE)
        if authors:
            metadata_dict["extracted_info"]["potential_authors"] = authors
            metadata_dict["author"] = authors[0]
            break
    
    # 5. DetectÄƒm versiuni sau numere de capitol
    version_patterns = [
        r'\b[vV](\d+(?:\.\d+)*)\b',  # v1.0, V2.1.3
        r'\b(?:version|ver)[\s:]*(\d+(?:\.\d+)*)\b',  # version 1.0, ver: 2.1
        r'\b(?:chapter|ch|cap)[\s:]*(\d+)\b',  # chapter 1, ch: 5
        r'\b(?:section|sec)[\s:]*(\d+(?:\.\d+)*)\b'  # section 1.2
    ]
    
    for pattern in version_patterns:
        versions = re.findall(pattern, metadata_string, re.IGNORECASE)
        if versions:
            metadata_dict["extracted_info"]["versions_or_chapters"] = versions
            metadata_dict["version"] = versions[0]
            break
    
    # 6. DetectÄƒm categorii sau tag-uri
    if '|' in metadata_string or ',' in metadata_string:
        # Probabil conÈ›ine tag-uri separate prin | sau ,
        separators = ['|', ',']
        for sep in separators:
            if sep in metadata_string:
                parts = [part.strip() for part in metadata_string.split(sep)]
                if len(parts) > 1:
                    metadata_dict["extracted_info"]["tags"] = parts
                    break
    
    # 7. DetectÄƒm URL-uri sau path-uri
    url_patterns = [
        r'https?://[^\s]+',
        r'www\.[^\s]+',
        r'[a-zA-Z0-9_-]+\.[a-zA-Z0-9_-]+(?:\.[a-zA-Z]{2,})?'
    ]
    
    for pattern in url_patterns:
        urls = re.findall(pattern, metadata_string)
        if urls:
            metadata_dict["extracted_info"]["urls_or_domains"] = urls
            metadata_dict["domain"] = urls[0]
            break
    
    # 8. CalculÄƒm un scor de completitudine pentru metadata
    info_count = len([v for v in metadata_dict["extracted_info"].values() if v])
    metadata_dict["metadata_richness"] = min(1.0, info_count / 5)  # Normalizat la 0-1
    
    # 9. AdÄƒugÄƒm sugestii pentru Ã®mbunÄƒtÄƒÈ›irea metadata
    suggestions = []
    if "document_name" not in metadata_dict:
        suggestions.append("AdÄƒugaÈ›i numele documentului")
    if "date_context" not in metadata_dict:
        suggestions.append("AdÄƒugaÈ›i informaÈ›ii despre datÄƒ")
    if "author" not in metadata_dict:
        suggestions.append("AdÄƒugaÈ›i informaÈ›ii despre autor")
    
    if suggestions:
        metadata_dict["improvement_suggestions"] = suggestions
    
    return metadata_dict

# FuncÈ›ii helper pentru debugging È™i testing - ÃMBUNÄ‚TÄ‚ÈšITE

def test_json_file(file_path: str) -> None:
    """
    TesteazÄƒ complet un fiÈ™ier JSON cu analiza Ã®mbunÄƒtÄƒÈ›itÄƒ È™i afiÈ™eazÄƒ rezultatele detaliate.
    """
    print(f"\nğŸ” TESTARE COMPLETÄ‚ FIÈ˜IER: {file_path}")
    print("=" * 70)
    
    start_time = time.time()
    
    # Test validare
    print("ğŸ“‹ VALIDARE FORMAT JSON...")
    is_valid, error_msg, chunks_count = validate_json_format(file_path)
    print(f"âœ… Validare: {'VALID' if is_valid else 'INVALID'}")
    if not is_valid:
        print(f"âŒ Eroare: {error_msg}")
        return
    
    print(f"ğŸ“Š Chunk-uri detectate: {chunks_count}")
    
    # Test statistici Ã®mbunÄƒtÄƒÈ›ite
    print("\nğŸ“ˆ STATISTICI DETALIATE...")
    stats = get_json_statistics(file_path)
    if 'error' not in stats:
        print(f"ğŸ“„ InformaÈ›ii fiÈ™ier:")
        print(f"   - Nume: {stats['file_info']['file_name']}")
        print(f"   - Dimensiune: {stats['file_info']['file_size_mb']} MB")
        
        print(f"ğŸ§© AnalizÄƒ chunk-uri:")
        ca = stats['chunk_analysis']
        print(f"   - Total: {ca['total_chunks']}")
        print(f"   - Valide: {ca['valid_chunks']}")
        print(f"   - Numerotare consistentÄƒ: {'DA' if ca['chunk_numbering_consistent'] else 'NU'}")
        
        print(f"ğŸ“ Statistici conÈ›inut:")
        cs = stats['content_statistics']
        print(f"   - Total cuvinte: {cs['total_words']:,}")
        print(f"   - Lungime medie chunk: {cs['average_chunk_length']} caractere")
        print(f"   - Cuvinte medii per chunk: {cs['average_words_per_chunk']}")
        print(f"   - Scor calitate medie: {cs['average_quality_score']}")
        
        print(f"ğŸŒ AnalizÄƒ metadata:")
        ma = stats['metadata_analysis']
        print(f"   - Surse unice: {ma['unique_metadata_sources']}")
        print(f"   - Limba dominantÄƒ: {ma['dominant_language']}")
        print(f"   - DistribuÈ›ie limbi: {ma['language_distribution']}")
        
        if stats['processing_recommendations']:
            print(f"ğŸ’¡ RecomandÄƒri:")
            for rec in stats['processing_recommendations']:
                print(f"   - {rec}")
    
    # Test previzualizare Ã®mbunÄƒtÄƒÈ›itÄƒ
    print("\nğŸ‘ï¸ PREVIZUALIZARE ÃMBUNÄ‚TÄ‚ÈšITÄ‚...")
    preview = preview_json_chunks(file_path, 2)
    if 'error' not in preview:
        pa = preview['preview_analysis']
        print(f"ğŸ“Š AnalizÄƒ previzualizare:")
        print(f"   - Calitate medie: {pa['average_quality']}")
        print(f"   - Limba dominantÄƒ: {pa['dominant_language']}")
        print(f"   - Total cuvinte preview: {pa['total_words_preview']}")
        
        print(f"ğŸ” Chunk-uri previzualizate:")
        for i, chunk in enumerate(preview['preview_chunks'], 1):
            print(f"   ğŸ“„ {chunk['chunk_id']}:")
            print(f"      - Cuvinte: {chunk['word_count']}")
            print(f"      - Calitate: {chunk['quality_score']}")
            print(f"      - LimbÄƒ: {chunk['language_detected']}")
            print(f"      - Keywords: {', '.join(chunk['keywords'])}")
            if chunk['processing_suggestions']:
                print(f"      - Sugestii: {'; '.join(chunk['processing_suggestions'])}")
    
    # Test procesare
    print("\nâš™ï¸ TEST PROCESARE OPTIMIZATÄ‚...")
    try:
        chunks_data = process_json_chunks(file_path)
        print(f"âœ… Procesare: {len(chunks_data)} chunk-uri procesate cu succes")
        
        # AnalizÄƒm calitatea chunk-urilor procesate
        quality_scores = [chunk['quality_score'] for chunk in chunks_data]
        avg_quality = sum(quality_scores) / len(quality_scores)
        high_quality = len([s for s in quality_scores if s > 0.7])
        
        print(f"ğŸ“Š Rezultate procesare:")
        print(f"   - Calitate medie: {avg_quality:.3f}")
        print(f"   - Chunk-uri de calitate Ã®naltÄƒ: {high_quality}/{len(chunks_data)}")
        
        # Sample din primul chunk procesat
        if chunks_data:
            sample_chunk = chunks_data[0]
            metadata = sample_chunk['metadata']
            print(f"ğŸ“‹ Sample metadata (primul chunk):")
            print(f"   - Keywords: {metadata.get('keywords', 'N/A')[:100]}...")
            print(f"   - Limba detectatÄƒ: {metadata.get('language_detected', 'N/A')}")
            print(f"   - Complexitate citire: {metadata.get('reading_complexity', 'N/A')}")
            print(f"   - Elemente structurale: {metadata.get('structural_elements', 0)}")
            
    except Exception as e:
        print(f"âŒ Eroare la procesare: {str(e)}")
    
    processing_time = time.time() - start_time
    print(f"\nâ±ï¸ TIMP TOTAL PROCESARE: {processing_time:.2f} secunde")
    print("=" * 70)

# Export pentru compatibilitate
__all__ = [
    'validate_json_format',
    'process_json_chunks', 
    'get_json_statistics',
    'preview_json_chunks',
    'validate_chunk_structure',
    'clean_chunk_content',
    'extract_metadata_info',
    'test_json_file',
    'clean_and_normalize_content',
    'extract_advanced_keywords',
    'detect_simple_language',
    'calculate_content_statistics',
    'extract_important_terms',
    'create_content_hash',
    'calculate_chunk_quality_score'
]"""
Utilitare OPTIMIZATE pentru procesarea fiÈ™ierelor JSON chunkizate
Versiunea 3.0.0 - ÃmbunÄƒtÄƒÈ›iri pentru cÄƒutare È™i procesare
"""

import json
import os
import logging
import re
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
from pathlib import Path
from collections import Counter
import hashlib

logger = logging.getLogger(__name__)

def validate_json_format(file_path: str) -> Tuple[bool, str, int]:
    """
    ValideazÄƒ cÄƒ fiÈ™ierul JSON are formatul EXACT specificat cu verificÄƒri Ã®mbunÄƒtÄƒÈ›ite.
    
    Formatul acceptat:
    {
        "chunk_0": {
            "metadata": "string",
            "chunk": "content"
        }
    }
    
    Returns:
        (is_valid, error_message, chunks_count)
    """
    try:
        if not os.path.exists(file_path):
            return False, "FiÈ™ierul nu existÄƒ", 0
        
        file_size = os.path.getsize(file_path)
        if file_size == 0:
            return False, "FiÈ™ierul este gol", 0
        
        if file_size > 100 * 1024 * 1024:  # 100MB limit
            return False, f"FiÈ™ierul este prea mare ({file_size // 1024 // 1024}MB). Maximum 100MB.", 0
        
        # ÃncercÄƒm sÄƒ Ã®ncÄƒrcÄƒm JSON-ul
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError as e:
                return False, f"JSON invalid: {str(e)[:100]}...", 0
        
        # VerificÄƒm cÄƒ este un dicÈ›ionar
        if not isinstance(data, dict):
            return False, "JSON-ul trebuie sÄƒ fie un obiect (dicÈ›ionar), nu o listÄƒ sau alt tip", 0
        
        if len(data) == 0:
            return False, "JSON-ul nu conÈ›ine date", 0
        
        # CÄƒutÄƒm chunk-uri Ã®n formatul chunk_X
        chunk_pattern = re.compile(r'^chunk_\d+$')
        chunk_keys = [key for key in data.keys() if chunk_pattern.match(key)]
        
        if len(chunk_keys) == 0:
            # ÃncercÄƒm sÄƒ gÄƒsim alte patterns comune
            other_keys = list(data.keys())[:10]  # Primele 10 chei pentru debugging
            return False, f"Nu s-au gÄƒsit chunk-uri Ã®n formatul aÈ™teptat (chunk_0, chunk_1, etc.). Chei gÄƒsite: {other_keys}", 0
        
        # ValidÄƒm structura chunk-urilor - verificÄƒm mai multe pentru siguranÈ›Äƒ
        valid_chunks = 0
        invalid_chunks = []
        sample_content_lengths = []
        
        for key in chunk_keys[:min(10, len(chunk_keys))]:  # VerificÄƒm pÃ¢nÄƒ la 10 chunk-uri
            chunk = data[key]
            
            # VerificÄƒm cÄƒ chunk-ul este un dicÈ›ionar
            if not isinstance(chunk, dict):
                invalid_chunks.append(f"{key}: nu este dicÈ›ionar")
                continue
            
            # VerificÄƒm cÄƒ are EXACT cheile: "metadata" È™i "chunk"
            required_keys = {"metadata", "chunk"}
            chunk_keys_set = set(chunk.keys())
            
            if not required_keys.issubset(chunk_keys_set):
                missing_keys = required_keys - chunk_keys_set
                invalid_chunks.append(f"{key}: lipsesc cheile {missing_keys}")
                continue
            
            # VerificÄƒm cÄƒ metadata este STRING
            if not isinstance(chunk["metadata"], str):
                invalid_chunks.append(f"{key}: metadata nu este string (este {type(chunk['metadata']).__name__})")
                continue
            
            if len(chunk["metadata"].strip()) == 0:
                invalid_chunks.append(f"{key}: metadata este string gol")
                continue
            
            # VerificÄƒm cÄƒ chunk este STRING cu conÈ›inut suficient
            if not isinstance(chunk["chunk"], str):
                invalid_chunks.append(f"{key}: chunk nu este string (este {type(chunk['chunk']).__name__})")
                continue
            
            content = chunk["chunk"].strip()
            if len(content) < 10:
                invalid_chunks.append(f"{key}: conÈ›inut prea scurt ({len(content)} caractere)")
                continue
            
            sample_content_lengths.append(len(content))
            valid_chunks += 1
        
        if valid_chunks == 0:
            error_details = "; ".join(invalid_chunks[:5])  # Primele 5 erori
            return False, f"Nu s-au gÄƒsit chunk-uri valide. Erori: {error_details}", 0
        
        # VerificÄƒm consistenÈ›a numerotÄƒrii chunk-urilor
        chunk_numbers = []
        for key in chunk_keys:
            match = re.match(r'chunk_(\d+)', key)
            if match:
                chunk_numbers.append(int(match.group(1)))
        
        chunk_numbers.sort()
        expected_sequence = list(range(len(chunk_numbers)))
        
        # Warning pentru numerotare inconsistentÄƒ (nu blochez validarea)
        if chunk_numbers != expected_sequence:
            logger.warning(f"Numerotarea chunk-urilor nu este consecutivÄƒ: gÄƒsite {chunk_numbers[:10]}..., aÈ™teptate {expected_sequence[:10]}...")
        
        # Statistici pentru logging
        avg_length = sum(sample_content_lengths) / len(sample_content_lengths) if sample_content_lengths else 0
        
        logger.info(f"âœ… JSON valid gÄƒsit cu {len(chunk_keys)} chunk-uri")
        logger.info(f"ğŸ“Š Validare: {valid_chunks}/{min(10, len(chunk_keys))} chunk-uri verificate cu succes")
        logger.info(f"ğŸ“ Lungime medie conÈ›inut: {avg_length:.0f} caractere")
        
        return True, f"Valid - {len(chunk_keys)} chunk-uri gÄƒsite", len(chunk_keys)
        
    except Exception as e:
        logger.error(f"Eroare la validarea JSON: {str(e)}")
        return False, f"Eroare neaÈ™teptatÄƒ la validare: {str(e)}", 0

def process_json_chunks(file_path: str) -> List[Dict[str, Any]]:
    """
    ProceseazÄƒ fiÈ™ierul JSON cu Ã®mbunÄƒtÄƒÈ›iri pentru cÄƒutare È™i indexare optimizatÄƒ.
    
    Returns:
        Lista de dicÈ›ionare cu chunk-uri procesate È™i Ã®mbunÄƒtÄƒÈ›ite
    """
    try:
        # ValidÄƒm mai Ã®ntÃ¢i formatul
        is_vali