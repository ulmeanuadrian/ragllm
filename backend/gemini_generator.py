import os
import time
import hashlib
import logging
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from collections import OrderedDict
from functools import lru_cache
import google.generativeai as genai
from dotenv import load_dotenv
import re
from collections import Counter

# Configurare logger
logger = logging.getLogger("rag_api")

# ÃncÄƒrcÄƒm variabilele de mediu
load_dotenv()

# ConfigurÄƒm API key pentru Gemini
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise ValueError("GOOGLE_API_KEY nu este setat Ã®n fiÈ™ierul .env")

genai.configure(api_key=GOOGLE_API_KEY)

# Singleton pentru a evita crearea mai multor instanÈ›e ale modelului
_gemini_model_instances = {}

# Cache pentru rÄƒspunsuri - optimizat cu LRU
_response_cache = OrderedDict()
MAX_CACHE_SIZE = 100  # LimitÄƒm dimensiunea cache-ului

# DicÈ›ionar general de sinonime È™i traduceri pentru diverse domenii
GENERAL_SYNONYMS = {
    # Termeni generali de cÄƒutare
    'cum': ['how', 'how to', 'Ã®n ce mod', 'Ã®n ce fel', 'modalitate'],
    'de ce': ['why', 'pentru ce', 'motivul', 'cauza', 'reason'],
    'ce': ['what', 'care', 'which', 'ce anume'],
    'unde': ['where', 'Ã®n care', 'location', 'locaÈ›ia'],
    'cÃ¢nd': ['when', 'momentul cÃ¢nd', 'timpul'],
    'cine': ['who', 'care persoanÄƒ'],
    'cÃ¢t': ['how much', 'how many', 'cÃ¢te', 'cantitate'],
    
    # Termeni tehnici generali
    'cod': ['code', 'script', 'program', 'coding', 'programare'],
    'script': ['script', 'cod', 'code', 'file', 'fiÈ™ier'],
    'functii': ['functions', 'funcÈ›ii', 'methods', 'metode'],
    'funcÈ›ii': ['functions', 'functii', 'methods', 'metode'],
    'componente': ['components', 'parts', 'pÄƒrÈ›i', 'elemente', 'elements'],
    'structura': ['structure', 'organizare', 'organization', 'structuring'],
    'organizare': ['organization', 'structura', 'structure', 'organize'],
    'configurare': ['configuration', 'config', 'setup', 'setare'],
    'instalare': ['installation', 'install', 'setup', 'installing'],
    'folosire': ['usage', 'using', 'utilizare', 'use'],
    'utilizare': ['usage', 'using', 'folosire', 'use'],
    
    # Termeni de dezvoltare
    'aplicatie': ['application', 'app', 'aplicaÈ›ie', 'program'],
    'aplicaÈ›ie': ['application', 'app', 'aplicatie', 'program'],
    'proiect': ['project', 'proyect', 'aplicaÈ›ie', 'application'],
    'dezvoltare': ['development', 'developing', 'creating', 'dev'],
    'crearea': ['creating', 'creation', 'making', 'building'],
    'construire': ['building', 'construction', 'creating'],
    
    # Termeni de fiÈ™iere È™i directoare
    'fisiere': ['files', 'fiÈ™iere', 'documents', 'documente'],
    'fiÈ™iere': ['files', 'fisiere', 'documents', 'documente'],
    'directoare': ['directories', 'folders', 'dosare', 'foldere'],
    'foldere': ['folders', 'directories', 'directoare'],
    'dosare': ['folders', 'directories', 'directoare', 'foldere'],
    
    # Termeni de documentaÈ›ie
    'documentatie': ['documentation', 'docs', 'documentaÈ›ie'],
    'documentaÈ›ie': ['documentation', 'docs', 'documentatie'],
    'ghid': ['guide', 'tutorial', 'manual'],
    'tutorial': ['tutorial', 'guide', 'ghid', 'walkthrough'],
    'exemplu': ['example', 'sample', 'demo', 'demonstration'],
    'exemple': ['examples', 'samples', 'demos', 'demonstrations'],
    
    # Termeni de proces
    'pas': ['step', 'phase', 'stage', 'etapÄƒ'],
    'pasi': ['steps', 'phases', 'stages', 'etape'],
    'paÈ™i': ['steps', 'phases', 'stages', 'etape'],
    'etapa': ['stage', 'phase', 'step', 'pas'],
    'etape': ['stages', 'phases', 'steps', 'paÈ™i'],
    'proces': ['process', 'procedure', 'procedurÄƒ'],
    'procedura': ['procedure', 'process', 'proces'],
    'procedurÄƒ': ['procedure', 'process', 'proces'],
    
    # Termeni de calitate È™i optimizare
    'optimizare': ['optimization', 'optimize', 'Ã®mbunÄƒtÄƒÈ›ire'],
    'imbunatatire': ['improvement', 'optimization', 'Ã®mbunÄƒtÄƒÈ›ire'],
    'Ã®mbunÄƒtÄƒÈ›ire': ['improvement', 'optimization', 'imbunatatire'],
    'performanta': ['performance', 'performanÈ›Äƒ', 'speed', 'viteza'],
    'performanÈ›Äƒ': ['performance', 'performanta', 'speed', 'viteza'],
    'calitate': ['quality', 'standard', 'nivel'],
    'eficienta': ['efficiency', 'eficienÈ›Äƒ', 'productive'],
    'eficienÈ›Äƒ': ['efficiency', 'eficienta', 'productive'],
    
    # Termeni de erori È™i probleme
    'eroare': ['error', 'mistake', 'greÈ™ealÄƒ', 'bug'],
    'erori': ['errors', 'mistakes', 'greÈ™eli', 'bugs'],
    'problema': ['problem', 'issue', 'problemÄƒ'],
    'probleme': ['problems', 'issues', 'probleme'],
    'solutie': ['solution', 'fix', 'soluÈ›ie'],
    'soluÈ›ie': ['solution', 'fix', 'solutie'],
    'rezolvare': ['solving', 'resolution', 'fix'],
    
    # Termeni de testare
    'testare': ['testing', 'test', 'verificare'],
    'test': ['test', 'testing', 'verificare'],
    'verificare': ['verification', 'check', 'testing'],
    'validare': ['validation', 'verify', 'confirmare'],
    
    # Termeni de management
    'gestionare': ['management', 'managing', 'handling'],
    'management': ['management', 'gestionare', 'administrare'],
    'administrare': ['administration', 'management', 'gestionare'],
    'control': ['control', 'management', 'supervision'],
}

# Stop words Ã®n romÃ¢nÄƒ È™i englezÄƒ (expandate)
STOP_WORDS_RO = {
    'È™i', 'Ã®n', 'la', 'de', 'cu', 'pe', 'din', 'pentru', 'este', 'sunt', 'a', 'al', 'ale',
    'cÄƒ', 'sÄƒ', 'se', 'nu', 'mai', 'dar', 'sau', 'dacÄƒ', 'cÃ¢nd', 'cum', 'unde', 'care',
    'aceastÄƒ', 'acest', 'aceasta', 'acesta', 'unei', 'unui', 'Ã®i', 'le', 'o', 'un',
    'am', 'ai', 'au', 'avea', 'fi', 'fost', 'fiind', 'va', 'vor', 'avea', 'aÈ™', 'ar',
    'cÄƒtre', 'despre', 'dupÄƒ', 'Ã®nainte', 'asupra', 'printre', 'Ã®ntre', 'sub', 'peste',
    'foarte', 'mult', 'puÈ›in', 'destul', 'aproape', 'departe', 'aici', 'acolo', 'undeva'
}

STOP_WORDS_EN = {
    'and', 'in', 'to', 'of', 'with', 'on', 'from', 'for', 'is', 'are', 'the', 'a', 'an',
    'that', 'this', 'it', 'be', 'have', 'has', 'do', 'does', 'will', 'would', 'could',
    'should', 'may', 'might', 'can', 'must', 'shall', 'at', 'by', 'up', 'as', 'if',
    'what', 'when', 'where', 'who', 'why', 'how', 'which', 'than', 'or', 'but', 'not',
    'was', 'were', 'been', 'being', 'had', 'having', 'about', 'into', 'through', 'during',
    'before', 'after', 'above', 'below', 'between', 'among', 'through', 'during', 'before'
}

ALL_STOP_WORDS = STOP_WORDS_RO.union(STOP_WORDS_EN)


class GeminiGenerator:
    def __init__(self, model_name: str = "gemini-2.5-flash"):
        """
        IniÈ›ializeazÄƒ generatorul Gemini optimizat pentru utilizare generalÄƒ.
        
        Args:
            model_name: Numele modelului Gemini de utilizat (implicit: gemini-2.5-flash)
        """
        self.model_name = model_name
        self._initialize_model()
    
    def _initialize_model(self):
        """IniÈ›ializeazÄƒ modelul pentru a fi pregÄƒtit de utilizare."""
        global _gemini_model_instances
        
        if self.model_name not in _gemini_model_instances:
            logger.info(f"Crearea unei noi instanÈ›e pentru modelul {self.model_name}")
            try:
                model_instance = genai.GenerativeModel(self.model_name)
                # Test iniÈ›ializare cu o interogare simplÄƒ
                _ = model_instance.generate_content("Test iniÈ›ializare")
                _gemini_model_instances[self.model_name] = model_instance
                logger.info(f"Modelul {self.model_name} a fost iniÈ›ializat cu succes.")
            except Exception as e:
                logger.error(f"Eroare la iniÈ›ializarea modelului: {e}")
                raise
        else:
            logger.info(f"Folosirea instanÈ›ei existente pentru modelul {self.model_name}")
            
        self.model = _gemini_model_instances[self.model_name]
    
    @staticmethod
    def expand_query_terms(query: str) -> List[str]:
        """
        ExpandeazÄƒ termenii din interogare cu sinonime È™i traduceri generale.
        
        Args:
            query: Interogarea originalÄƒ
            
        Returns:
            Lista de termeni expandaÈ›i
        """
        # NormalizÄƒm query-ul
        query_lower = query.lower().strip()
        
        # Extragem cuvintele folosind regex pentru a pÄƒstra È™i caracterele speciale
        words = re.findall(r'\b\w+\b', query_lower)
        
        # Lista de termeni expandaÈ›i
        expanded_terms = set(words)  # Ãncepem cu cuvintele originale
        
        # AdÄƒugÄƒm sinonime È™i traduceri din dicÈ›ionarul general
        for word in words:
            if word in GENERAL_SYNONYMS:
                expanded_terms.update(GENERAL_SYNONYMS[word])
            
            # CÄƒutÄƒm È™i Ã®n valorile dicÈ›ionarului (reverse lookup)
            for key, values in GENERAL_SYNONYMS.items():
                if word in values:
                    expanded_terms.add(key)
                    expanded_terms.update(values)
        
        # EliminÄƒm stop words
        expanded_terms = [term for term in expanded_terms if term not in ALL_STOP_WORDS]
        
        # EliminÄƒm cuvintele prea scurte
        expanded_terms = [term for term in expanded_terms if len(term) > 2]
        
        # EliminÄƒm duplicatele È™i sortÄƒm pentru consistenÈ›Äƒ
        expanded_terms = sorted(list(set(expanded_terms)))
        
        logger.debug(f"Query expandat: '{query}' -> {len(expanded_terms)} termeni: {expanded_terms[:10]}...")
        return expanded_terms
    
    @staticmethod
    def calculate_advanced_similarity(query: str, content: str) -> Dict[str, float]:
        """
        CalculeazÄƒ multiple tipuri de similaritate pentru o evaluare mai precisÄƒ.
        
        Args:
            query: Interogarea utilizatorului
            content: ConÈ›inutul documentului
            
        Returns:
            DicÈ›ionar cu diferite scoruri de similaritate
        """
        # 1. KEYWORD SIMILARITY - pe baza cuvintelor expandate
        query_terms = set(GeminiGenerator.expand_query_terms(query))
        content_words = set(re.findall(r'\b\w+\b', content.lower()))
        content_words = {word for word in content_words if len(word) > 2 and word not in ALL_STOP_WORDS}
        
        if not query_terms or not content_words:
            keyword_score = 0.0
        else:
            intersection = query_terms.intersection(content_words)
            union = query_terms.union(content_words)
            keyword_score = len(intersection) / len(union) if union else 0.0
        
        # 2. PHRASE SIMILARITY - cÄƒutare de fraze exacte
        phrase_score = 0.0
        query_phrases = [phrase.strip() for phrase in query.split(',') if len(phrase.strip()) > 3]
        content_lower = content.lower()
        
        for phrase in query_phrases:
            phrase_lower = phrase.lower()
            if phrase_lower in content_lower:
                # Bonus mai mare pentru fraze mai lungi
                phrase_bonus = min(0.3, len(phrase) / 50)
                phrase_score += phrase_bonus
        
        phrase_score = min(1.0, phrase_score)  # Cap la 1.0
        
        # 3. TERM FREQUENCY - cÃ¢t de des apar termenii cheie
        tf_score = 0.0
        for term in query_terms:
            if term in content_lower:
                # CalculÄƒm frecvenÈ›a relativÄƒ
                count = content_lower.count(term)
                tf_score += min(0.1, count / 100)  # NormalizÄƒm
        
        tf_score = min(1.0, tf_score)
        
        # 4. POSITION SCORE - termenii la Ã®nceputul documentului sunt mai importanÈ›i
        position_score = 0.0
        content_start = content_lower[:500]  # Primele 500 caractere
        
        for term in query_terms:
            if term in content_start:
                position_score += 0.1
        
        position_score = min(1.0, position_score)
        
        # 5. LENGTH BONUS - documentele cu lungime moderatÄƒ sunt preferate
        length_score = 0.0
        content_length = len(content)
        if 200 <= content_length <= 3000:  # Lungime optimÄƒ
            length_score = 0.2
        elif 100 <= content_length < 200 or 3000 < content_length <= 5000:
            length_score = 0.1
        
        scores = {
            'keyword': keyword_score,
            'phrase': phrase_score,
            'term_frequency': tf_score,
            'position': position_score,
            'length': length_score
        }
        
        # CalculÄƒm scorul final combinat cu ponderi
        weights = {
            'keyword': 0.4,      # 40% - cel mai important
            'phrase': 0.25,      # 25% - fraze exacte sunt importante
            'term_frequency': 0.2, # 20% - frecvenÈ›a termilor
            'position': 0.1,     # 10% - poziÈ›ia Ã®n document
            'length': 0.05       # 5% - lungimea documentului
        }
        
        final_score = sum(scores[key] * weights[key] for key in scores)
        scores['final'] = min(1.0, final_score)
        
        if scores['final'] > 0.1:  # Log doar pentru potriviri semnificative
            logger.debug(f"Similaritate: {scores['final']:.3f} (k:{scores['keyword']:.2f}, p:{scores['phrase']:.2f}, tf:{scores['term_frequency']:.2f})")
        
        return scores
    
    @staticmethod
    def _generate_cache_key(query: str, document_contents: List[str], temperature: float = 0.2) -> str:
        """
        GenereazÄƒ o cheie unicÄƒ pentru cache bazatÄƒ pe interogare È™i conÈ›inutul documentelor.
        
        Args:
            query: Interogarea utilizatorului
            document_contents: Lista conÈ›inuturilor documentelor
            temperature: Temperatura pentru generare
            
        Returns:
            Cheia pentru cache
        """
        # NormalizÄƒm interogarea
        query_normalized = query.lower().strip()
        
        # CreÄƒm un hash din interogare È™i conÈ›inutul documentelor
        hash_input = f"{query_normalized}|temp:{temperature}"
        
        # AdÄƒugÄƒm fingerprint-uri pentru conÈ›inutul documentelor
        for content in document_contents[:5]:  # LuÄƒm doar primele 5 pentru performanÈ›Äƒ
            # Folosim doar primele 200 de caractere pentru eficienÈ›Äƒ
            content_sample = content[:200].strip()
            content_hash = hashlib.md5(content_sample.encode()).hexdigest()[:8]
            hash_input += f"|{content_hash}"
        
        # GenerÄƒm hash-ul final
        return hashlib.md5(hash_input.encode()).hexdigest()
    
    @staticmethod
    def _extract_document_contents(documents: List[Dict[str, Any]]) -> List[str]:
        """
        Extrage conÈ›inutul documentelor Ã®n mod optimizat.
        
        Args:
            documents: Lista de documente
        
        Returns:
            Lista de conÈ›inuturi ale documentelor
        """
        contents = []
        for doc in documents:
            if isinstance(doc, dict):
                content = doc.get("content", "")
            else:
                content = getattr(doc, "content", "")
            
            if content and content.strip():  # IgnorÄƒm conÈ›inutul gol
                contents.append(content.strip())
        
        return contents
    
    @staticmethod
    def _build_enhanced_prompt(query: str, document_contents: List[str]) -> str:
        """
        ConstruieÈ™te un prompt Ã®mbunÄƒtÄƒÈ›it È™i general pentru Gemini.
        
        Args:
            query: Ãntrebarea utilizatorului
            document_contents: ConÈ›inuturile documentelor
        
        Returns:
            Prompt-ul pentru Gemini
        """
        # CreÈ™tem numÄƒrul de documente pentru context mai bogat
        max_docs = 10  # CreÈ™tem de la 5 la 10
        limited_contents = document_contents[:max_docs]
        
        # ExpandÄƒm termenii din query pentru context
        expanded_terms = GeminiGenerator.expand_query_terms(query)
        
        prompt = f"""EÈ™ti un asistent AI expert specializat Ã®n analiza documentelor È™i rÄƒspunsul la Ã®ntrebÄƒri Ã®n limba romÃ¢nÄƒ. Ai acces la un set de documente care pot conÈ›ine informaÈ›ii Ã®n romÃ¢nÄƒ sau englezÄƒ.

ÃNTREBARE UTILIZATOR: {query}

TERMENI CHEIE IDENTIFICAÈšI: {', '.join(expanded_terms[:15])}

CONTEXT DIN DOCUMENTE:"""
        
        for i, content in enumerate(limited_contents, 1):
            # CreÈ™tem lungimea permisÄƒ pentru fiecare document
            max_content_length = 4000  # CreÈ™tem pentru mai mult context
            truncated_content = content[:max_content_length]
            if len(content) > max_content_length:
                truncated_content += "...[content truncated]"
            
            # CalculÄƒm relevanÈ›a pentru acest document
            similarity_scores = GeminiGenerator.calculate_advanced_similarity(query, content)
            relevance = similarity_scores['final']
            
            prompt += f"\n\n--- DOCUMENT {i} (RelevanÈ›Äƒ: {relevance:.1%}) ---\n{truncated_content}"
        
        prompt += f"""

--- INSTRUCÈšIUNI PENTRU RÄ‚SPUNS ---
ğŸ¯ OBIECTIV: RÄƒspunde Ã®n limba romÃ¢nÄƒ la Ã®ntrebarea utilizatorului bazÃ¢ndu-te EXCLUSIV pe informaÈ›iile din documentele de mai sus.

ğŸ“‹ REGULI OBLIGATORII:
1. âœ… RÄƒspunde Ã®n romÃ¢nÄƒ, Ã®ntr-un mod clar, profesional È™i detaliat
2. ğŸ“š FoloseÈ™te DOAR informaÈ›iile din documentele furnizate
3. ğŸ” DacÄƒ gÄƒseÈ™ti informaÈ›ii relevante, explicÄƒ-le cu exemple concrete din documente
4. âŒ DacÄƒ informaÈ›iile nu sunt suficiente, spune clar: "Pe baza documentelor disponibile, nu pot oferi informaÈ›ii complete despre..."
5. ğŸ—ï¸ StructureazÄƒ rÄƒspunsul logic: introducere â†’ puncte principale â†’ exemple â†’ concluzie
6. ğŸ’¡ OferÄƒ sfaturi practice cÃ¢nd informaÈ›iile permit
7. ğŸ”— FÄƒ conexiuni Ã®ntre informaÈ›ii din diferite documente cÃ¢nd este relevant
8. ğŸ“Š FoloseÈ™te bullet points sau numerotare pentru claritate cÃ¢nd este necesar
9. âš¡ Fii concis dar complet - evitÄƒ informaÈ›ii redundante

ğŸ¨ STRUCTURA RÄ‚SPUNSULUI:
- Ãncepe cu un rezumat scurt (1-2 propoziÈ›ii)
- DezvoltÄƒ punctele principale cu detalii din documente
- Ãncheie cu o concluzie practicÄƒ

âš ï¸ NU INVENTA informaÈ›ii care nu sunt Ã®n documente!
âš ï¸ NU folosi cunoÈ™tinÈ›e generale - doar ce este Ã®n context!

RÄ‚SPUNS DETALIAT ÃN ROMÃ‚NÄ‚:"""
        
        return prompt
    
    @staticmethod
    def _add_to_cache(key: str, value: str) -> None:
        """
        AdaugÄƒ un rÄƒspuns Ã®n cache cu gestionare optimizatÄƒ a memoriei.
        
        Args:
            key: Cheia pentru cache
            value: Valoarea de stocat (rÄƒspunsul generat)
        """
        global _response_cache, MAX_CACHE_SIZE
        
        # DacÄƒ cache-ul a atins dimensiunea maximÄƒ, eliminÄƒm cea mai veche intrare
        if len(_response_cache) >= MAX_CACHE_SIZE:
            # OrderedDict pÄƒstreazÄƒ ordinea inserÄƒrii, deci prima cheie este cea mai veche
            _response_cache.popitem(last=False)  # FIFO (first in, first out)
        
        # AdÄƒugÄƒm noul rÄƒspuns Ã®n cache
        _response_cache[key] = {
            'value': value,
            'timestamp': time.time(),
            'access_count': 0
        }
        logger.debug(f"RÄƒspuns adÄƒugat Ã®n cache pentru cheia: {key[:16]}...")
    
    @staticmethod
    def _get_from_cache(key: str) -> Optional[str]:
        """
        ObÈ›ine un rÄƒspuns din cache.
        
        Args:
            key: Cheia pentru cache
            
        Returns:
            RÄƒspunsul din cache sau None dacÄƒ nu existÄƒ
        """
        global _response_cache
        
        if key in _response_cache:
            # ActualizÄƒm statisticile de acces
            cached_item = _response_cache.pop(key)
            cached_item['access_count'] += 1
            cached_item['last_access'] = time.time()
            
            # Reintroducem la sfÃ¢rÈ™it pentru LRU behavior
            _response_cache[key] = cached_item
            
            logger.debug(f"Cache hit pentru cheia: {key[:16]}... (accesat de {cached_item['access_count']} ori)")
            return cached_item['value']
        
        logger.debug(f"Cache miss pentru cheia: {key[:16]}...")
        return None
    
    def generate_response(
        self, 
        query: str, 
        context_docs: List[Dict[str, Any]], 
        temperature: float = 0.3,  # CreÈ™tem de la 0.2 la 0.3 pentru mai multÄƒ creativitate
        max_output_tokens: int = 1500,  # CreÈ™tem pentru rÄƒspunsuri mai detaliate
        top_k: int = 40,
        top_p: float = 0.95,
        use_web_search: bool = False,
    ) -> str:
        """
        GenereazÄƒ un rÄƒspuns la o Ã®ntrebare folosind documentele recuperate.
        OPTIMIZAT pentru orice tip de conÈ›inut JSON chunkizat.
        
        Args:
            query: Ãntrebarea utilizatorului
            context_docs: Lista de documente relevante
            temperature: Temperatura pentru generare (0.3 pentru echilibru creativitate/precizie)
            max_output_tokens: NumÄƒrul maxim de tokeni pentru rÄƒspuns
            top_k: Parametrul top_k pentru generare
            top_p: Parametrul top_p pentru generare
            use_web_search: DacÄƒ sÄƒ se foloseascÄƒ cÄƒutarea web (ignorat Ã®n aceastÄƒ versiune)
            
        Returns:
            RÄƒspunsul generat
        """
        # ValidÄƒri input
        if not query or not query.strip():
            logger.warning("Interogare goalÄƒ primitÄƒ")
            return "Interogarea nu poate fi goalÄƒ."
        
        if not context_docs:
            logger.warning("Nu s-au gÄƒsit documente relevante pentru interogare", 
                         extra={"query": query[:50]})
            return "Nu am gÄƒsit informaÈ›ii relevante pentru a rÄƒspunde la aceastÄƒ Ã®ntrebare. ÃncercaÈ›i sÄƒ reformulaÈ›i Ã®ntrebarea sau sÄƒ verificaÈ›i dacÄƒ documentele din colecÈ›ie conÈ›in informaÈ›iile cÄƒutate."
        
        # Extragem conÈ›inutul documentelor - creÈ™tem limita
        document_contents = self._extract_document_contents(context_docs[:10])  # CreÈ™tem de la 5 la 10
        
        if not document_contents:
            logger.warning("Documentele nu conÈ›in conÈ›inut valid", 
                         extra={"query": query[:50]})
            return "Documentele furnizate nu conÈ›in informaÈ›ii valide."
        
        # GenerÄƒm un hash pentru interogare È™i documente pentru cache
        cache_key = self._generate_cache_key(query, document_contents, temperature)
        
        # VerificÄƒm dacÄƒ rÄƒspunsul este Ã®n cache
        cached_response = self._get_from_cache(cache_key)
        if cached_response:
            logger.info("RÄƒspuns gÄƒsit Ã®n cache", 
                       extra={"query": query[:50], "cache_hit": True})
            return cached_response
        
        # Construim prompt-ul pentru Gemini
        prompt = self._build_enhanced_prompt(query, document_contents)
        
        try:
            # GenerÄƒm rÄƒspunsul cu mÄƒsurarea timpului
            start_time = time.time()
            
            generation_config = {
                "temperature": max(0.0, min(1.0, temperature)),  # Clamp la [0,1]
                "top_p": max(0.0, min(1.0, top_p)),
                "top_k": max(1, min(100, top_k)),
                "max_output_tokens": max(200, min(3000, max_output_tokens)),  # CreÈ™tem limita maximÄƒ
            }
            
            response = self.model.generate_content(
                prompt,
                generation_config=generation_config
            )
            
            generation_time = time.time() - start_time
            
            # LogÄƒm metricile de performanÈ›Äƒ
            logger.info("Generare rÄƒspuns Gemini finalizatÄƒ", 
                       extra={
                           "duration": f"{generation_time:.2f}s", 
                           "duration_ms": int(generation_time * 1000),
                           "query_length": len(query),
                           "doc_count": len(context_docs),
                           "prompt_length": len(prompt),
                           "temperature": temperature,
                           "model": self.model_name
                       })
            
            # Extragem textul rÄƒspunsului
            if not response or not response.text:
                logger.error("RÄƒspuns gol primit de la Gemini")
                return "Nu am putut genera un rÄƒspuns. Acest lucru se poate Ã®ntÃ¢mpla dacÄƒ Ã®ntrebarea este prea complexÄƒ sau documentele nu conÈ›in suficiente informaÈ›ii relevante. ÃncercaÈ›i sÄƒ reformulaÈ›i Ã®ntrebarea."
            
            answer = response.text.strip()
            
            # ValidÄƒm cÄƒ rÄƒspunsul nu este prea scurt
            if len(answer) < 20:  # CreÈ™tem pragul de la 10 la 20
                logger.warning("RÄƒspuns prea scurt generat de Gemini", 
                             extra={"answer_length": len(answer), "answer_preview": answer[:100]})
                return f"RÄƒspunsul generat este prea scurt. Pe baza documentelor disponibile, am gÄƒsit informaÈ›ii limitate. ÃncercaÈ›i sÄƒ reformulaÈ›i Ã®ntrebarea pentru a obÈ›ine un rÄƒspuns mai detaliat.\n\nRÄƒspuns parÈ›ial: {answer}"
            
            # VerificÄƒm dacÄƒ rÄƒspunsul conÈ›ine informaÈ›ii utile
            if any(phrase in answer.lower() for phrase in ["nu pot", "nu am", "nu existÄƒ", "nu gÄƒsesc"]):
                logger.info("RÄƒspuns cu informaÈ›ii limitate generat")
                # Nu returnÄƒm eroare, ci lÄƒs