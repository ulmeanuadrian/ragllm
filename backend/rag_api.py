import os
import json
import time
import logging
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import hashlib
from fastapi import FastAPI, HTTPException, File, UploadFile, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import chromadb
from chromadb.config import Settings
import google.generativeai as genai
import aiofiles
from dotenv import load_dotenv
import colorlog
from utils import (
    process_json_chunks, 
    validate_json_format, 
    get_json_statistics,
    preview_json_chunks
)

load_dotenv()

# Configurare logging cu culori
def setup_logging():
    """ConfigureazÄƒ logging-ul cu culori pentru debugging mai uÈ™or"""
    handler = colorlog.StreamHandler()
    handler.setFormatter(colorlog.ColoredFormatter(
        '%(log_color)s%(levelname)-8s%(reset)s %(blue)s%(name)s%(reset)s: %(message)s',
        log_colors={
            'DEBUG': 'cyan',
            'INFO': 'green',
            'WARNING': 'yellow',
            'ERROR': 'red',
            'CRITICAL': 'red,bg_white',
        }
    ))
    
    logger = logging.getLogger()
    logger.addHandler(handler)
    logger.setLevel(getattr(logging, os.getenv('LOG_LEVEL', 'INFO')))
    
    # Reduce logging pentru biblioteci externe
    logging.getLogger('chromadb').setLevel(logging.WARNING)
    
    return logger

logger = setup_logging()

# Configurare aplicaÈ›ie
app = FastAPI(
    title="RAG API - JSON Chunkizat (Simplificat)",
    description="API pentru Retrieval-Augmented Generation cu fiÈ™iere JSON chunkizate - Versiune simplificatÄƒ",
    version="2.1.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configurare CORS pentru frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Ãn producÈ›ie, specificaÈ›i domeniile exacte
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ConfigurÄƒri globale
CONFIG = {
    "CHROMA_PERSIST_DIR": os.getenv("CHROMA_PERSIST_DIRECTORY", "./chroma_db"),
    "GENERATIVE_MODEL": os.getenv("GENERATIVE_MODEL", "gemini-pro"),
    "MAX_FILE_SIZE": int(os.getenv("MAX_FILE_SIZE_MB", 50)) * 1024 * 1024,
    "DEFAULT_TOP_K": int(os.getenv("DEFAULT_TOP_K", 5)),
    "DEFAULT_TEMPERATURE": float(os.getenv("DEFAULT_TEMPERATURE", 0.2))
}

# InstanÈ›e globale
chroma_client = None
genai_model = None

# Modele Pydantic pentru validare
class QueryRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=1000)
    top_k_docs: int = Field(default=5, ge=1, le=20)
    temperature: float = Field(default=0.2, ge=0.0, le=1.0)

class QueryResponse(BaseModel):
    query: str
    answer: str
    documents: List[Dict[str, Any]]
    metadata: Dict[str, Any]

class CollectionInfo(BaseModel):
    name: str
    document_count: int
    total_chunks: int
    created_at: str

class DocumentInfo(BaseModel):
    source: str
    doc_count: int
    created_at: str
    file_size: Optional[int] = None

class DeleteDocumentRequest(BaseModel):
    source: str

# FuncÈ›ii helper pentru embeddings simple
def simple_text_hash(text: str) -> str:
    """CreeazÄƒ un hash simplu pentru text (Ã®nlocuieÈ™te embeddings-urile)"""
    return hashlib.md5(text.encode()).hexdigest()

def extract_keywords(text: str, max_keywords: int = 10) -> List[str]:
    """Extrage cuvinte cheie simple din text"""
    # CurÄƒÈ›Äƒm textul
    import re
    text = re.sub(r'[^\w\s]', ' ', text.lower())
    words = text.split()
    
    # FiltrÄƒm cuvintele comune
    stop_words = {
        'È™i', 'Ã®n', 'la', 'de', 'cu', 'pe', 'din', 'pentru', 'este', 'sunt', 'a', 'al', 'ale',
        'cÄƒ', 'sÄƒ', 'se', 'nu', 'mai', 'dar', 'sau', 'dacÄƒ', 'cÃ¢nd', 'cum', 'unde', 'care',
        'and', 'in', 'to', 'of', 'with', 'on', 'from', 'for', 'is', 'are', 'the', 'a', 'an',
        'that', 'this', 'it', 'be', 'have', 'has', 'do', 'does', 'will', 'would', 'could'
    }
    
    # NumÄƒrÄƒm frecvenÈ›a cuvintelor (excluzÃ¢nd stop words È™i cuvinte scurte)
    word_freq = {}
    for word in words:
        if len(word) > 2 and word not in stop_words:
            word_freq[word] = word_freq.get(word, 0) + 1
    
    # SortÄƒm dupÄƒ frecvenÈ›Äƒ
    keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
    return [word for word, freq in keywords[:max_keywords]]

def calculate_text_similarity(text1: str, text2: str) -> float:
    """CalculeazÄƒ similaritatea Ã®ntre douÄƒ texte folosind keywords overlap"""
    keywords1 = set(extract_keywords(text1))
    keywords2 = set(extract_keywords(text2))
    
    if not keywords1 or not keywords2:
        return 0.0
    
    # CalculÄƒm Jaccard similarity
    intersection = len(keywords1.intersection(keywords2))
    union = len(keywords1.union(keywords2))
    
    return intersection / union if union > 0 else 0.0

# FuncÈ›ii de iniÈ›ializare
async def initialize_services():
    """IniÈ›ializeazÄƒ serviciile necesare pentru RAG"""
    global chroma_client, genai_model
    
    try:
        logger.info("ğŸš€ IniÈ›ializare servicii RAG simplificat...")
        
        # 1. IniÈ›ializare ChromaDB
        logger.info("ğŸ“Š Conectare la ChromaDB...")
        persist_dir = Path(CONFIG["CHROMA_PERSIST_DIR"])
        persist_dir.mkdir(exist_ok=True)
        
        chroma_client = chromadb.PersistentClient(
            path=str(persist_dir),
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        logger.info(f"âœ… ChromaDB iniÈ›ializat Ã®n: {persist_dir}")
        
        # 2. Configurare Google Gemini
        google_api_key = os.getenv("GOOGLE_API_KEY")
        if not google_api_key or google_api_key == "your_google_api_key_here":
            logger.error("âŒ GOOGLE_API_KEY nu este configurat Ã®n .env!")
            logger.error("   ObÈ›ine un API key de la: https://makersuite.google.com/app/apikey")
            raise ValueError("Google API Key nu este configurat")
        
        logger.info("ğŸ”‘ Configurare Google Gemini...")
        genai.configure(api_key=google_api_key)
        genai_model = genai.GenerativeModel(CONFIG["GENERATIVE_MODEL"])
        
        # Test rapid Gemini
        try:
            test_response = genai_model.generate_content("Test")
            logger.info("âœ… Google Gemini configurat È™i funcÈ›ional")
        except Exception as e:
            logger.warning(f"âš ï¸ Test Gemini: {str(e)} (poate fi temporar)")
        
        logger.info("ğŸ‰ Serviciile au fost iniÈ›ializate cu succes!")
        
    except Exception as e:
        logger.error(f"âŒ Eroare la iniÈ›ializarea serviciilor: {str(e)}")
        raise

def get_or_create_collection(collection_name: str):
    """ObÈ›ine sau creeazÄƒ o colecÈ›ie ChromaDB"""
    try:
        # ÃncercÄƒm sÄƒ obÈ›inem colecÈ›ia existentÄƒ
        collection = chroma_client.get_collection(name=collection_name)
        logger.debug(f"ğŸ“ ColecÈ›ie existentÄƒ gÄƒsitÄƒ: {collection_name}")
    except Exception:
        # DacÄƒ nu existÄƒ, o creÄƒm
        collection = chroma_client.create_collection(
            name=collection_name,
            metadata={"created_at": datetime.now().isoformat()}
        )
        logger.info(f"ğŸ“ ColecÈ›ie nouÄƒ creatÄƒ: {collection_name}")
    
    return collection

async def process_and_store_chunks(collection_name: str, chunks_data: List[Dict[str, Any]], source_file: str):
    """ProceseazÄƒ È™i stocheazÄƒ chunk-urile Ã®n ChromaDB (fÄƒrÄƒ embeddings complexe)"""
    if not chunks_data:
        raise ValueError("Nu existÄƒ chunk-uri de procesat")
    
    collection = get_or_create_collection(collection_name)
    
    # PregÄƒtim datele pentru stocare
    documents = []
    metadatas = []
    ids = []
    
    for i, chunk_data in enumerate(chunks_data):
        content = chunk_data["content"]
        metadata = chunk_data["metadata"]
        
        # CreÄƒm un ID unic pentru chunk
        chunk_id = f"{source_file}_{metadata.get('chunk_id', f'chunk_{i}')}"
        
        documents.append(content)
        ids.append(chunk_id)
        
        # Metadata pentru ChromaDB
        chunk_metadata = {
            "source": source_file,
            "chunk_id": metadata.get("chunk_id", f"chunk_{i}"),
            "original_source": metadata.get("original_source", ""),
            "created_at": metadata.get("created_at", datetime.now().isoformat()),
            "content_length": len(content),
            "chunk_index": i,
            "keywords": ", ".join(extract_keywords(content, 5))  # SalvÄƒm keywords pentru cÄƒutare
        }
        metadatas.append(chunk_metadata)
    
    # StocÄƒm Ã®n ChromaDB (fÄƒrÄƒ embeddings - ChromaDB va folosi default embeddings)
    logger.info(f"ğŸ’¾ Stocare chunk-uri Ã®n colecÈ›ia '{collection_name}'...")
    collection.add(
        documents=documents,
        metadatas=metadatas,
        ids=ids
    )
    
    logger.info(f"âœ… {len(chunks_data)} chunk-uri stocate cu succes!")
    return len(chunks_data)

def search_relevant_chunks(collection_name: str, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """CautÄƒ chunk-uri relevante pentru query (folosind ChromaDB default search)"""
    try:
        collection = chroma_client.get_collection(name=collection_name)
    except Exception:
        raise HTTPException(status_code=404, detail=f"ColecÈ›ia '{collection_name}' nu existÄƒ")
    
    # CÄƒutÄƒm chunk-uri similare folosind ChromaDB default search
    results = collection.query(
        query_texts=[query],
        n_results=top_k,
        include=["documents", "metadatas", "distances"]
    )
    
    # FormatÄƒm rezultatele
    relevant_chunks = []
    if results["documents"] and results["documents"][0]:
        for i, (doc, metadata, distance) in enumerate(zip(
            results["documents"][0],
            results["metadatas"][0], 
            results["distances"][0]
        )):
            # Convertim distanÈ›a Ã®n scor de similaritate (0-1)
            similarity_score = max(0, 1 - distance)
            
            # CalculÄƒm È™i un scor suplimentar bazat pe keywords
            keyword_similarity = calculate_text_similarity(query, doc)
            combined_score = (similarity_score + keyword_similarity) / 2
            
            relevant_chunks.append({
                "content": doc,
                "meta": metadata,
                "score": combined_score,
                "rank": i + 1,
                "match_type": "semantic" if similarity_score > 0.5 else "keyword"
            })
    
    # SortÄƒm dupÄƒ scorul combinat
    relevant_chunks.sort(key=lambda x: x["score"], reverse=True)
    
    return relevant_chunks

async def generate_answer_with_context(query: str, relevant_chunks: List[Dict[str, Any]], temperature: float = 0.2) -> str:
    """GenereazÄƒ rÄƒspuns folosind Google Gemini cu context din chunk-uri"""
    if not relevant_chunks:
        return "Nu am gÄƒsit informaÈ›ii relevante pentru aceastÄƒ Ã®ntrebare."
    
    # Construim contextul din chunk-uri
    context_parts = []
    for i, chunk in enumerate(relevant_chunks, 1):
        source = chunk["meta"].get("original_source", "NecunoscutÄƒ")
        content = chunk["content"][:800]  # LimitÄƒm lungimea pentru a nu depÄƒÈ™i limitele
        score = chunk["score"]
        context_parts.append(f"[SursÄƒ {i}: {source} - RelevanÈ›Äƒ: {score:.1%}]\n{content}")
    
    context = "\n\n".join(context_parts)
    
    # Prompt optimizat pentru Gemini
    prompt = f"""EÈ™ti un asistent AI specializat Ã®n analiza documentelor JSON. 

BazÃ¢ndu-te EXCLUSIV pe informaÈ›iile din contextul de mai jos, rÄƒspunde la Ã®ntrebarea utilizatorului Ã®ntr-un mod clar, concis È™i util.

CONTEXT:
{context}

ÃNTREBARE: {query}

INSTRUCÈšIUNI:
- RÄƒspunde doar pe baza informaÈ›iilor din context
- DacÄƒ informaÈ›iile nu sunt suficiente, spune acest lucru
- Fii precis È™i oferÄƒ exemple concrete din context cÃ¢nd este posibil
- OrganizeazÄƒ rÄƒspunsul Ã®ntr-un mod logic È™i uÈ™or de Ã®nÈ›eles
- Nu inventa informaÈ›ii care nu sunt Ã®n context
- MenÈ›ioneazÄƒ sursa informaÈ›iilor cÃ¢nd este relevant

RÄ‚SPUNS:"""
    
    try:
        # ConfigurÄƒm parametrii de generare
        generation_config = genai.types.GenerationConfig(
            temperature=temperature,
            top_p=0.95,
            top_k=64,
            max_output_tokens=1000,
        )
        
        response = genai_model.generate_content(
            prompt,
            generation_config=generation_config
        )
        
        return response.text.strip()
        
    except Exception as e:
        logger.error(f"Eroare la generarea rÄƒspunsului cu Gemini: {str(e)}")
        return f"Eroare la generarea rÄƒspunsului: {str(e)}"

# Endpoints API (identice cu versiunea complexÄƒ)

@app.on_event("startup")
async def startup_event():
    """IniÈ›ializare la pornirea aplicaÈ›iei"""
    await initialize_services()

@app.get("/health")
async def health_check():
    """Verificare sÄƒnÄƒtate API"""
    return {
        "status": "healthy",
        "version": "simplified",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "chromadb": chroma_client is not None,
            "gemini": genai_model is not None,
            "embeddings": "ChromaDB default (simple)"
        }
    }

@app.get("/collections", response_model=List[str])
async def list_collections():
    """ListeazÄƒ toate colecÈ›iile disponibile"""
    try:
        collections = chroma_client.list_collections()
        collection_names = [col.name for col in collections]
        logger.debug(f"ğŸ“ GÄƒsite {len(collection_names)} colecÈ›ii")
        return collection_names
    except Exception as e:
        logger.error(f"Eroare la listarea colecÈ›iilor: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/collections/{collection_name}")
async def create_collection(collection_name: str):
    """CreeazÄƒ o colecÈ›ie nouÄƒ"""
    try:
        # Validare nume colecÈ›ie
        if not collection_name.replace("_", "").isalnum():
            raise HTTPException(status_code=400, detail="Numele colecÈ›iei poate conÈ›ine doar litere, cifre È™i underscore")
        
        # VerificÄƒm dacÄƒ colecÈ›ia existÄƒ deja
        try:
            chroma_client.get_collection(name=collection_name)
            raise HTTPException(status_code=409, detail=f"ColecÈ›ia '{collection_name}' existÄƒ deja")
        except Exception:
            # ColecÈ›ia nu existÄƒ, o putem crea
            pass
        
        collection = chroma_client.create_collection(
            name=collection_name,
            metadata={"created_at": datetime.now().isoformat()}
        )
        
        logger.info(f"ğŸ“ ColecÈ›ie creatÄƒ: {collection_name}")
        return {"message": f"ColecÈ›ia '{collection_name}' a fost creatÄƒ cu succes"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Eroare la crearea colecÈ›iei: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/collections/{collection_name}")
async def delete_collection(collection_name: str):
    """È˜terge o colecÈ›ie È™i toate documentele ei"""
    try:
        chroma_client.delete_collection(name=collection_name)
        logger.info(f"ğŸ—‘ï¸ ColecÈ›ie È™tearsÄƒ: {collection_name}")
        return {"message": f"ColecÈ›ia '{collection_name}' a fost È™tearsÄƒ cu succes"}
    except Exception as e:
        logger.error(f"Eroare la È™tergerea colecÈ›iei: {str(e)}")
        raise HTTPException(status_code=404, detail=f"ColecÈ›ia '{collection_name}' nu existÄƒ")

@app.get("/collections/{collection_name}/documents", response_model=List[DocumentInfo])
async def list_documents(collection_name: str):
    """ListeazÄƒ documentele dintr-o colecÈ›ie"""
    try:
        collection = chroma_client.get_collection(name=collection_name)
        
        # ObÈ›inem toate documentele È™i grupÄƒm dupÄƒ sursÄƒ
        all_docs = collection.get(include=["metadatas"])
        
        # GrupÄƒm dupÄƒ sursÄƒ
        sources = {}
        for metadata in all_docs["metadatas"]:
            source = metadata.get("source", "NecunoscutÄƒ")
            if source not in sources:
                sources[source] = {
                    "source": source,
                    "doc_count": 0,
                    "created_at": metadata.get("created_at", ""),
                    "file_size": metadata.get("file_size")
                }
            sources[source]["doc_count"] += 1
        
        result = list(sources.values())
        logger.debug(f"ğŸ“„ GÄƒsite {len(result)} surse Ã®n colecÈ›ia {collection_name}")
        return result
        
    except Exception as e:
        logger.error(f"Eroare la listarea documentelor: {str(e)}")
        raise HTTPException(status_code=404, detail=f"ColecÈ›ia '{collection_name}' nu existÄƒ")

@app.post("/collections/{collection_name}/upload")
async def upload_document(collection_name: str, file: UploadFile = File(...)):
    """ÃncarcÄƒ È™i proceseazÄƒ un fiÈ™ier JSON chunkizat"""
    start_time = time.time()
    
    try:
        # ValidÄƒri fiÈ™ier
        if not file.filename.lower().endswith('.json'):
            raise HTTPException(status_code=400, detail="Doar fiÈ™ierele JSON sunt acceptate")
        
        # VerificÄƒ dimensiunea fiÈ™ierului
        content = await file.read()
        if len(content) > CONFIG["MAX_FILE_SIZE"]:
            raise HTTPException(status_code=413, detail=f"FiÈ™ierul este prea mare. Maximum {CONFIG['MAX_FILE_SIZE']//1024//1024}MB")
        
        # SalvÄƒm temporar fiÈ™ierul
        temp_file_path = f"temp_{int(time.time())}_{file.filename}"
        async with aiofiles.open(temp_file_path, 'wb') as f:
            await f.write(content)
        
        # ValidÄƒm formatul JSON
        is_valid, error_msg, chunks_count = validate_json_format(temp_file_path)
        if not is_valid:
            os.unlink(temp_file_path)  # È˜tergem fiÈ™ierul temporar
            raise HTTPException(status_code=400, detail=f"Format JSON invalid: {error_msg}")
        
        # ProcesÄƒm chunk-urile
        logger.info(f"ğŸ“„ Procesare fiÈ™ier: {file.filename} ({chunks_count} chunk-uri)")
        chunks_data = process_json_chunks(temp_file_path)
        
        # StocÄƒm Ã®n ChromaDB
        stored_chunks = await process_and_store_chunks(collection_name, chunks_data, file.filename)
        
        # CurÄƒÈ›Äƒm fiÈ™ierul temporar
        os.unlink(temp_file_path)
        
        processing_time = time.time() - start_time
        
        logger.info(f"âœ… FiÈ™ier procesat cu succes Ã®n {processing_time:.2f}s: {stored_chunks} chunk-uri")
        
        return {
            "message": f"FiÈ™ierul '{file.filename}' a fost procesat cu succes",
            "filename": file.filename,
            "chunks_count": stored_chunks,
            "processing_time": f"{processing_time:.2f}s"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Eroare la Ã®ncÄƒrcarea documentului: {str(e)}")
        # CurÄƒÈ›Äƒm fiÈ™ierul temporar Ã®n caz de eroare
        if 'temp_file_path' in locals() and os.path.exists(temp_file_path):
            os.unlink(temp_file_path)
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/collections/{collection_name}/documents")
async def delete_document(collection_name: str, request: DeleteDocumentRequest):
    """È˜terge un document din colecÈ›ie"""
    try:
        collection = chroma_client.get_collection(name=collection_name)
        
        # GÄƒsim toate chunk-urile din acest document
        results = collection.get(
            where={"source": request.source},
            include=["metadatas"]
        )
        
        if not results["ids"]:
            raise HTTPException(status_code=404, detail=f"Documentul '{request.source}' nu a fost gÄƒsit")
        
        # È˜tergem toate chunk-urile
        collection.delete(ids=results["ids"])
        
        logger.info(f"ğŸ—‘ï¸ Document È™ters: {request.source} ({len(results['ids'])} chunk-uri)")
        
        return {
            "message": f"Documentul '{request.source}' a fost È™ters cu succes",
            "deleted_chunks": len(results["ids"])
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Eroare la È™tergerea documentului: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/collections/{collection_name}/generate", response_model=QueryResponse)
async def generate_answer(collection_name: str, request: QueryRequest):
    """GenereazÄƒ rÄƒspuns pe baza unei Ã®ntrebÄƒri È™i a documentelor din colecÈ›ie"""
    start_time = time.time()
    
    try:
        logger.info(f"ğŸ” Procesare Ã®ntrebare: '{request.query[:50]}...' Ã®n colecÈ›ia '{collection_name}'")
        
        # CÄƒutÄƒm chunk-uri relevante
        relevant_chunks = search_relevant_chunks(
            collection_name=collection_name,
            query=request.query,
            top_k=request.top_k_docs
        )
        
        if not relevant_chunks:
            return QueryResponse(
                query=request.query,
                answer="Nu am gÄƒsit informaÈ›ii relevante pentru aceastÄƒ Ã®ntrebare Ã®n documentele disponibile.",
                documents=[],
                metadata={
                    "processing_time": f"{time.time() - start_time:.2f}s",
                    "chunks_found": 0,
                    "collection": collection_name,
                    "search_method": "simplified"
                }
            )
        
        # GenerÄƒm rÄƒspuns cu Gemini
        answer = await generate_answer_with_context(
            query=request.query,
            relevant_chunks=relevant_chunks,
            temperature=request.temperature
        )
        
        processing_time = time.time() - start_time
        
        logger.info(f"âœ… RÄƒspuns generat Ã®n {processing_time:.2f}s cu {len(relevant_chunks)} chunk-uri")
        
        return QueryResponse(
            query=request.query,
            answer=answer,
            documents=relevant_chunks,
            metadata={
                "processing_time": f"{processing_time:.2f}s",
                "chunks_found": len(relevant_chunks),
                "collection": collection_name,
                "temperature": request.temperature,
                "search_method": "simplified"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Eroare la generarea rÄƒspunsului: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/collections/{collection_name}/search")
async def search_documents(collection_name: str, query: str, top_k: int = 5):
    """CautÄƒ documente fÄƒrÄƒ generare de rÄƒspuns"""
    try:
        relevant_chunks = search_relevant_chunks(collection_name, query, top_k)
        
        return {
            "query": query,
            "results": relevant_chunks,
            "total_found": len(relevant_chunks),
            "search_method": "simplified"
        }
        
    except Exception as e:
        logger.error(f"Eroare la cÄƒutarea documentelor: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    
    # Configurare pentru rulare
    host = os.getenv("API_HOST", "0.0.0.0")
    port = int(os.getenv("API_PORT", 8070))
    debug = os.getenv("DEBUG", "True").lower() == "true"
    
    logger.info("=" * 50)
    logger.info("ğŸš€ RAG API pentru JSON Chunkizat (SIMPLIFICAT)")
    logger.info("=" * 50)
    logger.info(f"ğŸ“¡ Server: http://{host}:{port}")
    logger.info(f"ğŸ“š Docs: http://{host}:{port}/docs")
    logger.info(f"ğŸ”§ Debug: {debug}")
    logger.info(f"âš¡ Versiune: SimplificatÄƒ (fÄƒrÄƒ sentence-transformers)")
    logger.info("=" * 50)
    
    uvicorn.run(
        "rag_api:app",
        host=host,
        port=port,
        reload=debug,
        log_level="info" if debug else "warning"
    )